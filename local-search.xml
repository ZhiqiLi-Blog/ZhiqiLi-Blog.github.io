<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>(2021-10-15)Intelligent Vision Coursework</title>
    <link href="/2023/01/10/2021-10-15-Intelligent-Vision-Coursework/"/>
    <url>/2023/01/10/2021-10-15-Intelligent-Vision-Coursework/</url>
    
    <content type="html"><![CDATA[<h1 id="Use-of-Baser"><a href="#Use-of-Baser" class="headerlink" title="Use of Baser"></a>Use of Baser</h1><p>在该实验中主要通过opencv使用拍摄的棋盘格对相机进行calibration.求出相机的内参和外参.实验设备为basler工业相机.<br>注:opencv的sample中有标准示例程序,这里的程序是自己写的仅供练习.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sstream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;string&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;ctime&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/core.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/core/utility.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/imgproc.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/calib3d.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/imgcodecs.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/videoio.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/highgui.hpp&gt;</span></span><br><br>using namespace cv;<br>using namespace <span class="hljs-built_in">std</span>;<br><span class="hljs-built_in">string</span> imageList[]=&#123;<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n1.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n2.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n3.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n4.JPG&quot;</span>),\<br><span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n5.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n6.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n7.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n8.JPG&quot;</span>),\<br><span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n9.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n10.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n11.JPG&quot;</span>),<span class="hljs-built_in">string</span>(<span class="hljs-string">&quot;./n12.JPG&quot;</span>)<br>&#125;;<br>CvSize <span class="hljs-title function_">boardSize</span><span class="hljs-params">(<span class="hljs-number">5</span>,<span class="hljs-number">7</span>)</span>;<br><span class="hljs-type">int</span> photo_size=<span class="hljs-number">12</span>;<br><span class="hljs-type">float</span> squareSize=<span class="hljs-number">50</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br><span class="hljs-built_in">vector</span>&lt; Point3f &gt; CornerPoints;<br><span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt; Point3f &gt;&gt; ObjectPoints;<br><span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt; Point2f &gt;&gt; imagePoints;<br><span class="hljs-built_in">vector</span>&lt;Mat&gt; save_view;<br><span class="hljs-type">bool</span> have_set=<span class="hljs-literal">false</span>;<br>CvSize ImageSize;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;photo_size;i++)&#123;<br><span class="hljs-comment">//reference to argument 2:https://blog.csdn.net/qq_27278957/article/details/84589526</span><br>Mat view = imread(imageList[i], <span class="hljs-number">1</span>);<br><span class="hljs-built_in">cout</span>&lt;&lt;view.size()&lt;&lt;<span class="hljs-built_in">endl</span>;<br>         imshow(<span class="hljs-string">&quot;input&quot;</span>, view);<span class="hljs-comment">//显示</span><br>ImageSize=view.size();<br><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;cv::Point2f&gt; ptvec;<br><span class="hljs-type">int</span>  corner_count=<span class="hljs-number">9</span>; <br><span class="hljs-comment">//the error for find==0:https://blog.csdn.net/u011651743/article/details/51099543</span><br><span class="hljs-type">bool</span> found = findChessboardCorners( view, boardSize, ptvec ,CALIB_CB_ADAPTIVE_THRESH );<br><span class="hljs-comment">//for more precise ,use  cvFindCornerSubPix</span><br><span class="hljs-comment">/*</span><br><span class="hljs-comment">void cv::drawChessboardCorners(</span><br><span class="hljs-comment">cv::InputOutputArray image, // 棋盘格图像（8UC3）即是输入也是输出</span><br><span class="hljs-comment">cv::Size patternSize, // 棋盘格内部角点的行、列数</span><br><span class="hljs-comment">cv::InputArray corners, // findChessboardCorners()输出的角点</span><br><span class="hljs-comment">bool patternWasFound // findChessboardCorners()的返回值</span><br><span class="hljs-comment">);</span><br><span class="hljs-comment">*/</span><br>drawChessboardCorners(view,boardSize,ptvec,found);<br>imshow(<span class="hljs-string">&quot;input&quot;</span>, view);<br>save_view.push_back(view);<br>imagePoints.push_back(ptvec);<br><span class="hljs-keyword">if</span>(have_set==<span class="hljs-literal">false</span>)&#123;<br>have_set=<span class="hljs-literal">true</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;j&lt;boardSize.height;j++)&#123;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> k=<span class="hljs-number">0</span>;k&lt;boardSize.width;k++)&#123;<br>CornerPoints.push_back(Point3f(j*squareSize,k*squareSize,<span class="hljs-number">0</span>));<br>&#125;<br><br>&#125;<br>&#125;<br><br>&#125;<br>ObjectPoints.resize(imagePoints.size(),CornerPoints);<br><br>Mat cameraMatrix;<br>cameraMatrix= Mat::eye(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, CV_64F);<br>        cameraMatrix.at&lt;<span class="hljs-type">double</span>&gt;(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>) = <span class="hljs-number">1</span>;<br><br>Mat distCoeffs;<br>distCoeffs=Mat::zeros(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>, CV_64F);<br><span class="hljs-type">int</span> iFixedPoint = <span class="hljs-number">-1</span>;<br><br><span class="hljs-built_in">vector</span>&lt;Mat&gt; rvecs;<span class="hljs-built_in">vector</span>&lt;Mat&gt; tvecs;<br><br><span class="hljs-type">double</span> rms = calibrateCamera(ObjectPoints, imagePoints, ImageSize,<br>                        cameraMatrix, distCoeffs, rvecs, tvecs,<br>                         CALIB_FIX_ASPECT_RATIO  | CALIB_USE_LU);<br><span class="hljs-built_in">cout</span>&lt;&lt;cameraMatrix&lt;&lt;<span class="hljs-built_in">endl</span>;<br><span class="hljs-comment">//now calculate the actual point for each feature;</span><br><span class="hljs-type">double</span> totalErr=<span class="hljs-number">0</span>,totalPoints=<span class="hljs-number">0</span>,err;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;photo_size;i++)&#123;<br><span class="hljs-built_in">vector</span>&lt; Point2f &gt; projectedPoints;<br>cv::projectPoints(ObjectPoints[i], rvecs[i], tvecs[i],cameraMatrix,\<br>distCoeffs, projectedPoints);<br>err = norm(imagePoints[i], projectedPoints, NORM_L2);<br><span class="hljs-built_in">cout</span>&lt;&lt;i&lt;&lt;<span class="hljs-string">&quot;:&quot;</span>&lt;&lt;<span class="hljs-built_in">endl</span>;<br><span class="hljs-comment">//for(int k=0;k&lt;imagePoints[i].size();k++)&#123;</span><br><span class="hljs-comment">//cout&lt;&lt;imagePoints[i][k]&lt;&lt;&quot;:&quot;&lt;&lt;projectedPoints[k]&lt;&lt;endl;</span><br><span class="hljs-comment">//&#125;</span><br>drawChessboardCorners(save_view[i],boardSize,projectedPoints,<span class="hljs-literal">true</span>);<br>imshow(<span class="hljs-string">&quot;input2&quot;</span>, save_view[i]);cv::waitKey(<span class="hljs-number">1000</span>);<br><span class="hljs-built_in">cout</span>&lt;&lt;err&lt;&lt;<span class="hljs-built_in">endl</span>;<br>totalErr        += err*err;<br>        totalPoints     += ObjectPoints[i].size();<br><span class="hljs-comment">//Mat rotate_matrix;</span><br><span class="hljs-comment">//Rodrigues(rvecs[i], rotate_matrix, noArray());</span><br><span class="hljs-comment">//for(int j=0;j&lt;boardSize.width;j++)&#123;</span><br><span class="hljs-comment">//for(int k=0;k&lt;boardSize.height;k++)&#123;</span><br><span class="hljs-comment">//Mat p =(Mat_&lt;double&gt;(3,1)&lt;&lt;);</span><br><span class="hljs-comment">//&#125;</span><br><span class="hljs-comment">//&#125;</span><br><br><span class="hljs-comment">//cout&lt;&lt;rotate_matrix&lt;&lt;endl;</span><br><span class="hljs-comment">//cout&lt;&lt;rvecs[i].size()&lt;&lt;endl;</span><br>&#125;<br><span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">sqrt</span>(totalErr/totalPoints)&lt;&lt;<span class="hljs-built_in">endl</span>;<br><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h1><p>这是智能视觉采集大作业的主要程序</p><ul><li>目的:从视频拍摄中得到全景图,包括离线的和在线的.</li><li>过程:特征点提取匹配-图片射影变换-图片合成<br>  (1)特征点提取匹配:ORB算法<br>  (2)图片合成的方法分为两种,第一种是离线的,第二种是在线的(个人提出的拙劣的算法).<h2 id="1-离线合成"><a href="#1-离线合成" class="headerlink" title="1.离线合成"></a>1.离线合成</h2>离线部分和在线部分有一个不同的是图片合成算法.</li><li>对于在线合成,速度时比较重要的,所以这里对于前后两帧的图片采用<br>$$<br>Img_t(i,j)=max{newImg_t(i,j),Img_{t-1}(i,j)}<br>$$<br>采用这种算法的原因是,新的图像经过射影变换后,在图片上会留有黑色的空位.需要通过依旧求得的全景图的部分对其进行填充.</li><li>对于离线合成,精确度是比较重要的,所以这里采用下述算法<br>$$<br>Img_t(i,j)=\begin{cases}max{newImg_t(i,j),Img_{t-1}(i,j)},&amp;newImg_t(i,j)&lt;k \ \alpha newImg_t(i,j)+\beta Img_{t-1}(i,j),&amp;newImg_t(i,j)&gt;=k \end{cases}<br>$$<br>其中$\alpha+\beta=1$<br>由于图片经过变换后可能会产生部分没有对齐的情况,如果仍然采用离线的算法,将会导致黑色的物体在图像中渐渐的被掩盖掉(最典型的就是头发在全景图的生成过程中渐渐的消失).所以一般来说,在全景图的生成过程中都会使用新图和旧图乘一个系数合成的方法.同时我们仍然需要考虑到,新图经过射影变换后留有黑色空位的情况.所以,在该算法中,需要对新图的某个像素是黑色空位还是其他的情况进行区分,即设一个阈值k,用新图的像素与阈值进行比较.小于阈值,则进行加系数融合;如果大于阈值,则取最大值,消去黑色空位.</li></ul><p>离线部分代码</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><code class="hljs c"><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;sstream&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;string&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;ctime&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/opencv.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/core.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/core/utility.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/imgproc.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/calib3d.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/imgcodecs.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/videoio.hpp&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/highgui.hpp&gt;</span></span><br><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/video/background_segm.hpp&gt;</span></span><br><br>using namespace cv;<br>using namespace <span class="hljs-built_in">std</span>;<br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span>&#123;<br>    VideoCapture input=VideoCapture(<span class="hljs-string">&quot;input5.mp4&quot;</span>);<br>    <span class="hljs-keyword">if</span>(input.isOpened())&#123;<br>        <span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-string">&quot;input sucess&quot;</span>&lt;&lt;<span class="hljs-built_in">endl</span>;<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-built_in">cout</span>&lt;&lt;<span class="hljs-string">&quot;fail input&quot;</span>&lt;&lt;<span class="hljs-built_in">endl</span>;<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>    &#125;<br>    Mat view; <br>    <span class="hljs-type">int</span> cnt=<span class="hljs-number">0</span>;<br>    <span class="hljs-built_in">vector</span>&lt;Mat&gt; calibrate_images;<br>    <span class="hljs-built_in">vector</span>&lt;Mat&gt; calibrate_descriptors;<br>    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;KeyPoint&gt; &gt; calibrate_keypoints;<br>    Mat <span class="hljs-title function_">dst</span><span class="hljs-params">(<span class="hljs-number">720</span>+<span class="hljs-number">100</span>,<span class="hljs-number">1280</span>*<span class="hljs-number">6</span>, CV_8UC3)</span>;dst.setTo(<span class="hljs-number">0</span>);<br>    Mat save_homo;<br>    <span class="hljs-keyword">while</span>(<span class="hljs-number">1</span>)&#123;<br>        <span class="hljs-keyword">if</span>(!input.read(view)) <span class="hljs-keyword">break</span>;<br>        <span class="hljs-comment">//cout&lt;&lt;view.size&lt;&lt;endl;</span><br>        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;KeyPoint&gt; keypoints;<br>        Mat descriptors;<br>        Ptr&lt;FeatureDetector&gt; detector = ORB::create();<br>        Ptr&lt;DescriptorExtractor&gt; descriptor = ORB::create();<br>    <span class="hljs-comment">//Ptr&lt;DescriptorMatcher&gt; matcher  = DescriptorMatcher::create ( &quot;BruteForce-Hamming&quot; );</span><br>        detector-&gt;detect ( view,keypoints );<br>        descriptor-&gt;compute ( view, keypoints, descriptors );<br>        Mat outimg;<br>        calibrate_keypoints.push_back(keypoints);<br>        calibrate_descriptors.push_back(descriptors);<br>        drawKeypoints( view, keypoints, outimg, Scalar::all(<span class="hljs-number">-1</span>), DrawMatchesFlags::DEFAULT );<br>        <span class="hljs-comment">//imshow(&quot;ORB特征点&quot;,outimg);</span><br>        <span class="hljs-comment">//imshow(&quot;input&quot;, view);</span><br>        <span class="hljs-comment">//cv::waitKey(10000);</span><br>        <br>        calibrate_images.push_back(view);<br>        <span class="hljs-keyword">if</span>(view.rows==<span class="hljs-number">0</span>) <span class="hljs-keyword">break</span>;<br>        <span class="hljs-keyword">if</span>(cnt&gt;=<span class="hljs-number">1</span>)&#123;<br>            <span class="hljs-comment">//the first 15 images will be used calidate.</span><br>            Ptr&lt;DescriptorMatcher&gt; matcher  = DescriptorMatcher::create ( <span class="hljs-string">&quot;BruteForce-Hamming&quot;</span> );<br>            <span class="hljs-built_in">vector</span>&lt;DMatch&gt; matches;<br>            Mat img_match;<br>    <span class="hljs-comment">//BFMatcher matcher ( NORM_HAMMING );</span><br>            matcher-&gt;match ( calibrate_descriptors[cnt<span class="hljs-number">-1</span>],  calibrate_descriptors[cnt], matches );<br>            <span class="hljs-comment">//optimize</span><br>            <span class="hljs-type">double</span> min_dist=<span class="hljs-number">10000</span>, max_dist=<span class="hljs-number">0</span>;<br>            <span class="hljs-comment">//找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离</span><br>            <span class="hljs-keyword">for</span> ( <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; calibrate_descriptors[cnt<span class="hljs-number">-1</span>].rows; i++ )<br>            &#123;<br>                <span class="hljs-type">double</span> dist = matches[i].distance;<br>                <span class="hljs-keyword">if</span> ( dist &lt; min_dist ) min_dist = dist;<br>                <span class="hljs-keyword">if</span> ( dist &gt; max_dist ) max_dist = dist;<br>            &#125;<br>            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt; DMatch &gt; good_matches;<br>            <span class="hljs-keyword">for</span> ( <span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; calibrate_descriptors[cnt<span class="hljs-number">-1</span>].rows; i++ )<br>            &#123;<br>                <span class="hljs-keyword">if</span> ( matches[i].distance &lt;= max ( <span class="hljs-number">2</span>*min_dist, <span class="hljs-number">30.0</span> ) )<br>                &#123;<br>                    good_matches.push_back ( matches[i] );<br>                &#125;<br>            &#125;<br>            drawMatches ( calibrate_images[cnt<span class="hljs-number">-1</span>], calibrate_keypoints[cnt<span class="hljs-number">-1</span>], calibrate_images[cnt], calibrate_keypoints[cnt], good_matches, img_match );<br>            <span class="hljs-comment">//imshow(&quot;ORB特征点2&quot;,img_match);</span><br>            <br>            <span class="hljs-comment">//calibrate here</span><br>            <span class="hljs-comment">//**********************************************//</span><br>            <span class="hljs-built_in">vector</span>&lt;Point2f&gt; imagePoints1, imagePoints2;<br><br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i&lt;good_matches.size(); i++)<br>            &#123;<br>                imagePoints2.push_back(calibrate_keypoints[cnt<span class="hljs-number">-1</span>][good_matches[i].queryIdx].pt);<br>                imagePoints1.push_back(calibrate_keypoints[cnt][good_matches[i].trainIdx].pt);<br>            &#125;<br><br>            Mat homo = findHomography(imagePoints1, imagePoints2, CV_RANSAC);<br>            <span class="hljs-keyword">if</span>(cnt==<span class="hljs-number">1</span>) save_homo=homo.clone();<br>            <span class="hljs-keyword">else</span> save_homo=save_homo*homo;<br>            homo=save_homo;<br><br>           <span class="hljs-comment">// cout&lt;&lt;homo&lt;&lt;endl;</span><br>            Mat imageTransform;<br>            warpPerspective(calibrate_images[cnt], imageTransform, homo, Size(calibrate_images[cnt].cols+<span class="hljs-number">1000</span>, calibrate_images[cnt].rows+<span class="hljs-number">100</span>));<br>            <span class="hljs-comment">//imshow(&quot;直接经过透视矩阵变换&quot;, imageTransform);</span><br>            <span class="hljs-comment">//imwrite(&quot;trans1.jpg&quot;, imageTransform);</span><br>            <span class="hljs-type">int</span> dst_width = imageTransform.cols;  <span class="hljs-comment">//取最右点的长度为拼接图的长度</span><br>            <br>            <span class="hljs-type">int</span> dst_height =calibrate_images[cnt].rows;<br>            <br>  <br>            Mat <span class="hljs-title function_">tem_dst</span><span class="hljs-params">(<span class="hljs-number">720</span>+<span class="hljs-number">100</span>,<span class="hljs-number">1280</span>*<span class="hljs-number">6</span>, CV_8UC3)</span>;tem_dst.setTo(<span class="hljs-number">0</span>);<br>            imageTransform.copyTo(tem_dst(Rect(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, imageTransform.cols, imageTransform.rows)));<br>            Mat tem_dst2=dst.clone();<br>            <span class="hljs-comment">//calibrate_images[cnt].copyTo(dst(Rect(0, 0, calibrate_images[cnt].cols, calibrate_images[cnt].rows)));</span><br>            <span class="hljs-comment">//max(tem_dst,tem_dst2,dst);</span><br>            <span class="hljs-comment">//addWeighted(tem_dst, 0.1,tem_dst2,0.9, 0, dst);</span><br>            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; tem_dst.rows; j++) &#123;<br>                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>; k &lt; tem_dst.cols; k++) &#123;<br>                    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> m = <span class="hljs-number">0</span>; m &lt; <span class="hljs-number">3</span>; m++) &#123;<br>                        <span class="hljs-keyword">if</span> (tem_dst.at&lt;Vec3b&gt;(j, k)[m] &lt;= <span class="hljs-number">130</span>) &#123;<br>                            dst.at&lt;Vec3b&gt;(j, k)[m] = max(tem_dst2.at&lt;Vec3b&gt;(j, k)[m], tem_dst.at&lt;Vec3b&gt;(j, k)[m]);<br>                        &#125;<br>                        <span class="hljs-keyword">else</span> &#123;<br>                            dst.at&lt;Vec3b&gt;(j, k)[m] = (tem_dst2.at&lt;Vec3b&gt;(j, k)[m]*<span class="hljs-number">5</span>+ tem_dst.at&lt;Vec3b&gt;(j, k)[m]*<span class="hljs-number">5</span>)/<span class="hljs-number">10</span>;<br>                        &#125;<br>                    &#125;<br><span class="hljs-comment">//cout&lt;&lt; tem_dst.at</span><br>                    <span class="hljs-comment">//if(tem_dst.at)</span><br>                &#125;<br>            &#125;<br>        imshow(<span class="hljs-string">&quot;b_dst&quot;</span>, dst);cv::waitKey(<span class="hljs-number">1000</span>);    <br>        &#125;<br>        <br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;<span class="hljs-number">5</span>;i++)&#123;<br>            input.read(view);<br>        &#125;<br>        cnt++;<br>        <br>    &#125;<br>    <br>    cv::waitKey(<span class="hljs-number">1000000</span>);<br>    <span class="hljs-built_in">cout</span>&lt;&lt;cnt&lt;&lt;<span class="hljs-built_in">endl</span>;<br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="2-在线合成"><a href="#2-在线合成" class="headerlink" title="2.在线合成"></a>2.在线合成</h2><p>在线合成前面已经提到了图像融合的算法.除此之外,在线和成主要是使用basler相机的代码.代码如下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;pylon/PylonIncludes.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> PYLON_WIN_BUILD</span><br><span class="hljs-meta">#    <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;pylon/PylonGUI.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;opencv2/opencv.hpp&gt;</span></span><br><br><span class="hljs-comment">// Namespace for using pylon objects.</span><br>using namespace Pylon;<br><br><span class="hljs-comment">// Namespace for using cout.</span><br>using namespace <span class="hljs-built_in">std</span>;<br>using namespace cv;<br><br><span class="hljs-comment">// Number of images to be grabbed.</span><br><span class="hljs-type">static</span> <span class="hljs-type">const</span> <span class="hljs-type">uint32_t</span> c_countOfImagesToGrab = <span class="hljs-number">1000</span>;<br><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span>* argv[])</span><br>&#123;<br><span class="hljs-comment">// The exit code of the sample application.</span><br><span class="hljs-type">int</span> exitCode = <span class="hljs-number">0</span>;<br><br><span class="hljs-comment">// Before using any pylon methods, the pylon runtime must be initialized. </span><br>PylonInitialize();<br>Mat frame;<br><span class="hljs-comment">//****************************************************</span><br><span class="hljs-comment">//[2592 x 1944]</span><br>Mat view;<br><span class="hljs-type">int</span> cnt = <span class="hljs-number">0</span>;<br><span class="hljs-built_in">vector</span>&lt;Mat&gt; calibrate_images;<br><span class="hljs-built_in">vector</span>&lt;Mat&gt; calibrate_descriptors;<br><span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;KeyPoint&gt; &gt; calibrate_keypoints;<br>Mat <span class="hljs-title function_">dst</span><span class="hljs-params">(<span class="hljs-number">194</span> + <span class="hljs-number">50</span>, <span class="hljs-number">259</span> * <span class="hljs-number">10</span>, CV_8UC3)</span>; dst.setTo(<span class="hljs-number">0</span>);<br>Mat save_homo;<br><span class="hljs-comment">//****************************************************</span><br>try<br>&#123;<br><span class="hljs-comment">// Create an instant camera object with the camera device found first.</span><br>CInstantCamera <span class="hljs-title function_">camera</span><span class="hljs-params">(CTlFactory::GetInstance().CreateFirstDevice())</span>;<br><br><span class="hljs-comment">// Print the model name of the camera.</span><br><span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Using device &quot;</span> &lt;&lt; camera.GetDeviceInfo().GetModelName() &lt;&lt; <span class="hljs-built_in">endl</span>;<br><br><span class="hljs-comment">// The parameter MaxNumBuffer can be used to control the count of buffers</span><br><span class="hljs-comment">// allocated for grabbing. The default value of this parameter is 10.</span><br>camera.MaxNumBuffer = <span class="hljs-number">5</span>;<br><br><span class="hljs-comment">// Start the grabbing of c_countOfImagesToGrab images.</span><br><span class="hljs-comment">// The camera device is parameterized with a default configuration which</span><br><span class="hljs-comment">// sets up free-running continuous acquisition.</span><br>camera.StartGrabbing(c_countOfImagesToGrab);<br><br><span class="hljs-comment">// This smart pointer will receive the grab result data.</span><br>CGrabResultPtr ptrGrabResult;<br><br><span class="hljs-comment">/// new image that convert to cv::Mat</span><br>CImageFormatConverter formatConverter;<br>formatConverter.OutputPixelFormat = PixelType_BGR8packed;<br>CPylonImage pylonImage;<br><br><span class="hljs-comment">// Camera.StopGrabbing() is called automatically by the RetrieveResult() method</span><br><span class="hljs-comment">// when c_countOfImagesToGrab images have been retrieved.</span><br><span class="hljs-type">char</span> c;<br><span class="hljs-keyword">while</span> (c = waitKey(<span class="hljs-number">1</span>) &amp;&amp; camera.IsGrabbing())<br>&#123;<br><span class="hljs-comment">// Wait for an image and then retrieve it. A timeout of 5000 ms is used.</span><br>camera.RetrieveResult(<span class="hljs-number">5000</span>, ptrGrabResult, TimeoutHandling_ThrowException);<br><br><span class="hljs-comment">// Image grabbed successfully?</span><br><span class="hljs-keyword">if</span> (ptrGrabResult-&gt;GrabSucceeded())<br>&#123;<br><span class="hljs-comment">// Access the image data.</span><br><span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;SizeX: &quot;</span> &lt;&lt; ptrGrabResult-&gt;GetWidth() &lt;&lt; <span class="hljs-built_in">endl</span>;<br><span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;SizeY: &quot;</span> &lt;&lt; ptrGrabResult-&gt;GetHeight() &lt;&lt; <span class="hljs-built_in">endl</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">uint8_t</span> *pImageBuffer = (<span class="hljs-type">uint8_t</span> *)ptrGrabResult-&gt;GetBuffer();<br><span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Gray value of first pixel: &quot;</span> &lt;&lt; (<span class="hljs-type">uint32_t</span>)pImageBuffer[<span class="hljs-number">0</span>] &lt;&lt; <span class="hljs-built_in">endl</span> &lt;&lt; <span class="hljs-built_in">endl</span>;<br><br><span class="hljs-comment">//#ifdef PYLON_WIN_BUILD</span><br><span class="hljs-comment">// Display the grabbed image.</span><br><span class="hljs-comment">//Pylon::DisplayImage(1, ptrGrabResult);</span><br><span class="hljs-comment">//#endif</span><br><span class="hljs-comment">///convert to cv::Mat</span><br>formatConverter.Convert(pylonImage, ptrGrabResult);<br>frame = cv::Mat(ptrGrabResult-&gt;GetHeight(), ptrGrabResult-&gt;GetWidth(), CV_8UC3, (<span class="hljs-type">uint8_t</span> *)pylonImage.GetBuffer());<br><span class="hljs-comment">/// show</span><br><span class="hljs-comment">//resize(frame, frame, Size(frame.cols / 2, frame.rows / 2));</span><br><span class="hljs-built_in">cout</span> &lt;&lt; frame.size() &lt;&lt; <span class="hljs-built_in">endl</span>;<br>imshow(<span class="hljs-string">&quot;OpenCV Display Window&quot;</span>, frame);<br><span class="hljs-keyword">if</span>(<span class="hljs-literal">true</span>)<br>&#123;<br>Mat <span class="hljs-title function_">view</span><span class="hljs-params">(<span class="hljs-number">194</span>, <span class="hljs-number">259</span>, frame.type())</span>;<br>resize(frame, view, view.size(), <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, INTER_LINEAR);<br><span class="hljs-comment">//*****************************************************************************8</span><br><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;KeyPoint&gt; keypoints;<br>keypoints.resize(<span class="hljs-number">500</span>);<br>Mat descriptors;<br>Ptr&lt;FeatureDetector&gt; detector = ORB::create();<br>Ptr&lt;DescriptorExtractor&gt; descriptor = ORB::create();<br><span class="hljs-comment">//Ptr&lt;DescriptorMatcher&gt; matcher  = DescriptorMatcher::create ( &quot;BruteForce-Hamming&quot; );</span><br><span class="hljs-built_in">cout</span> &lt;&lt; view.size &lt;&lt; <span class="hljs-built_in">endl</span>;<br>detector-&gt;detect(view, keypoints);<br>descriptor-&gt;compute(view, keypoints, descriptors);<br>Mat outimg;<br>calibrate_keypoints.push_back(keypoints);<br>calibrate_descriptors.push_back(descriptors);<br>drawKeypoints(view, keypoints, outimg, Scalar::all(<span class="hljs-number">-1</span>), DrawMatchesFlags::DEFAULT);<br><span class="hljs-comment">//imshow(&quot;ORB特征点&quot;,outimg);</span><br><span class="hljs-comment">//imshow(&quot;input&quot;, view);</span><br><span class="hljs-comment">//cv::waitKey(10000);</span><br><br>calibrate_images.push_back(view);<br><span class="hljs-keyword">if</span> (cnt &gt;= <span class="hljs-number">1</span>) &#123;<br><span class="hljs-comment">//the first 15 images will be used calidate.</span><br>Ptr&lt;DescriptorMatcher&gt; matcher = DescriptorMatcher::create(<span class="hljs-string">&quot;BruteForce-Hamming&quot;</span>);<br><span class="hljs-built_in">vector</span>&lt;DMatch&gt; matches;<br>Mat img_match;<br><span class="hljs-comment">//BFMatcher matcher ( NORM_HAMMING );</span><br>matcher-&gt;match(calibrate_descriptors[cnt - <span class="hljs-number">1</span>], calibrate_descriptors[cnt], matches);<br><span class="hljs-comment">//optimize</span><br><span class="hljs-type">double</span> min_dist = <span class="hljs-number">10000</span>, max_dist = <span class="hljs-number">0</span>;<br><span class="hljs-comment">//找出所有匹配之间的最小距离和最大距离, 即是最相似的和最不相似的两组点之间的距离</span><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; calibrate_descriptors[cnt - <span class="hljs-number">1</span>].rows; i++)<br>&#123;<br><span class="hljs-type">double</span> dist = matches[i].distance;<br><span class="hljs-keyword">if</span> (dist &lt; min_dist) min_dist = dist;<br><span class="hljs-keyword">if</span> (dist &gt; max_dist) max_dist = dist;<br>&#125;<br><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt; DMatch &gt; good_matches;<br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; calibrate_descriptors[cnt - <span class="hljs-number">1</span>].rows; i++)<br>&#123;<br><span class="hljs-keyword">if</span> (matches[i].distance &lt;= max(<span class="hljs-number">2</span> * min_dist, <span class="hljs-number">30.0</span>))<br>&#123;<br>good_matches.push_back(matches[i]);<br>&#125;<br>&#125;<br>drawMatches(calibrate_images[cnt - <span class="hljs-number">1</span>], calibrate_keypoints[cnt - <span class="hljs-number">1</span>], calibrate_images[cnt], calibrate_keypoints[cnt], good_matches, img_match);<br><span class="hljs-comment">//imshow(&quot;ORB特征点2&quot;,img_match);</span><br><br><span class="hljs-comment">//calibrate here</span><br><span class="hljs-comment">//**********************************************//</span><br><span class="hljs-built_in">vector</span>&lt;Point2f&gt; imagePoints1, imagePoints2;<br><br><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; good_matches.size(); i++)<br>&#123;<br>imagePoints2.push_back(calibrate_keypoints[cnt - <span class="hljs-number">1</span>][good_matches[i].queryIdx].pt);<br>imagePoints1.push_back(calibrate_keypoints[cnt][good_matches[i].trainIdx].pt);<br>&#125;<br><br>Mat homo = findHomography(imagePoints1, imagePoints2, CV_RANSAC);<br><span class="hljs-keyword">if</span> (cnt == <span class="hljs-number">1</span>) save_homo = homo.clone();<br><span class="hljs-keyword">else</span> save_homo = save_homo * homo;<br>homo = save_homo;<br><br><span class="hljs-comment">// cout&lt;&lt;homo&lt;&lt;endl;</span><br>Mat imageTransform;<br>warpPerspective(calibrate_images[cnt], imageTransform, homo, Size(calibrate_images[cnt].cols + <span class="hljs-number">1000</span>, calibrate_images[cnt].rows + <span class="hljs-number">50</span>));<br><span class="hljs-comment">//imshow(&quot;直接经过透视矩阵变换&quot;, imageTransform);</span><br><span class="hljs-comment">//imwrite(&quot;trans1.jpg&quot;, imageTransform);</span><br><span class="hljs-type">int</span> dst_width = imageTransform.cols;  <span class="hljs-comment">//取最右点的长度为拼接图的长度</span><br><br><span class="hljs-type">int</span> dst_height = calibrate_images[cnt].rows;<br><br><br>Mat <span class="hljs-title function_">tem_dst</span><span class="hljs-params">(<span class="hljs-number">194</span> + <span class="hljs-number">50</span>,  <span class="hljs-number">259</span> * <span class="hljs-number">10</span>, CV_8UC3)</span>; tem_dst.setTo(<span class="hljs-number">0</span>);<br>imageTransform.copyTo(tem_dst(Rect(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, imageTransform.cols, imageTransform.rows)));<br>Mat tem_dst2 = dst.clone();<br><span class="hljs-comment">//calibrate_images[cnt].copyTo(dst(Rect(0, 0, calibrate_images[cnt].cols, calibrate_images[cnt].rows)));</span><br>max(tem_dst, tem_dst2, dst);<br><span class="hljs-comment">/*for (int j = 0; j &lt; tem_dst.rows; j++) &#123;</span><br><span class="hljs-comment">for (int k = 0; k &lt; tem_dst.cols; k++) &#123;</span><br><span class="hljs-comment">for (int m = 0; m &lt; 3; m++) &#123;</span><br><span class="hljs-comment">if (tem_dst.at&lt;Vec3b&gt;(j, k)[m] &lt;= 10) &#123;</span><br><span class="hljs-comment">dst.at&lt;Vec3b&gt;(j, k)[m] = max(tem_dst2.at&lt;Vec3b&gt;(j, k)[m], tem_dst.at&lt;Vec3b&gt;(j, k)[m]);</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">else &#123;</span><br><span class="hljs-comment">dst.at&lt;Vec3b&gt;(j, k)[m] = tem_dst2.at&lt;Vec3b&gt;(j, k)[m]*0.3+ tem_dst.at&lt;Vec3b&gt;(j, k)[m]*0.7;</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">//cout&lt;&lt; tem_dst.at</span><br><span class="hljs-comment">//if(tem_dst.at)</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">&#125;</span><br><span class="hljs-comment">*/</span><br><span class="hljs-comment">//max(tem_dst,0)</span><br><span class="hljs-comment">//cout &lt;&lt; dst &lt;&lt; endl;</span><br><br><span class="hljs-comment">//addWeighted(tem_dst, 0.3,tem_dst2,0.7, 0, dst);</span><br><br>imshow(<span class="hljs-string">&quot;b_dst&quot;</span>, dst);<br>&#125;<br><br>&#125;<br>cnt++;<br><br><br><span class="hljs-comment">//******************************************************************************</span><br>&#125;<br><span class="hljs-keyword">else</span><br>&#123;<br><span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Error: &quot;</span> &lt;&lt; ptrGrabResult-&gt;GetErrorCode() &lt;&lt; <span class="hljs-string">&quot; &quot;</span> &lt;&lt; ptrGrabResult-&gt;GetErrorDescription() &lt;&lt; <span class="hljs-built_in">endl</span>;<br>&#125;<br>&#125;<br>&#125;<br>catch (<span class="hljs-type">const</span> GenericException &amp;e)<br>&#123;<br><span class="hljs-comment">// Error handling.</span><br><span class="hljs-built_in">cerr</span> &lt;&lt; <span class="hljs-string">&quot;An exception occurred.&quot;</span> &lt;&lt; <span class="hljs-built_in">endl</span><br>&lt;&lt; e.GetDescription() &lt;&lt; <span class="hljs-built_in">endl</span>;<br>exitCode = <span class="hljs-number">1</span>;<br>&#125;<br><br><span class="hljs-comment">// Comment the following two lines to disable waiting on exit.</span><br><span class="hljs-built_in">cerr</span> &lt;&lt; <span class="hljs-built_in">endl</span> &lt;&lt; <span class="hljs-string">&quot;Press Enter to exit.&quot;</span> &lt;&lt; <span class="hljs-built_in">endl</span>;<br><span class="hljs-keyword">while</span> (<span class="hljs-built_in">cin</span>.get() != <span class="hljs-string">&#x27;\n&#x27;</span>);<br><br><span class="hljs-comment">// Releases all pylon resources. </span><br>PylonTerminate();<br><br><span class="hljs-keyword">return</span> exitCode;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="3结果"><a href="#3结果" class="headerlink" title="3结果"></a>3结果</h2><p><img src="https://img-blog.csdnimg.cn/1e915609e65349de8562b708f2e4ff5b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    
    <tags>
      
      <tag>CV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-10-10)3D-Reconstruction Review(4)</title>
    <link href="/2023/01/10/2021-10-10-3D-Reconstruction-Review-4/"/>
    <url>/2023/01/10/2021-10-10-3D-Reconstruction-Review-4/</url>
    
    <content type="html"><![CDATA[<h1 id="Microfacet-Models-for-Refraction-through-Rough-Surfaces"><a href="#Microfacet-Models-for-Refraction-through-Rough-Surfaces" class="headerlink" title="Microfacet Models for Refraction through Rough Surfaces"></a>Microfacet Models for Refraction through Rough Surfaces</h1><p>这篇文章中提到了一个很重要的模型GGX,前面的几篇文章中都用了该模型.</p><h1 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1.introduction"></a>1.introduction</h1><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h1><p>参见论文.</p><h1 id="3-Microfacet-Theory"><a href="#3-Microfacet-Theory" class="headerlink" title="3.Microfacet Theory"></a>3.Microfacet Theory</h1><ul><li><strong>If restricted to only reflection or transmission,it is often called the BRDF or BTDF, respectively, and our BSDF will be the sum of a BRDF,</strong>  $f_r$, <strong>and a BTDF</strong>, $f_t$ ,<strong>term. Since we want to include both reflection and transmission as</strong> $f_s(i,o,n)$</li><li>In microfacet models, a detailed microsurface is replaced by a simplified macrosurface (see Figure 4) with a modified scattering function (BSDF) that matches the aggregate directional scattering of the microsurface (i.e. both should appear the same from a distance).即microsurface的细节太小了,只有far-field directional scattering pattern是重要的,同时忽略wave effect,只考虑几何光学,only single scattering is modeled<br><img src="https://img-blog.csdnimg.cn/4e95edef22b349f0873874ff206dc77b.png" alt="在这里插入图片描述">- 它假设了microsurface可以被两个量描述:distribution function D and a shadowing-masking function G, together with a microsurface BSDF $f_s$<h2 id="3-1-Microfacet-Distribution-Function-D"><a href="#3-1-Microfacet-Distribution-Function-D" class="headerlink" title="3.1 Microfacet Distribution Function, D"></a>3.1 Microfacet Distribution Function, D</h2></li><li>D($\bold{m}$), describes the statistical distribution of surface normals $\bold{m}$ over the microsurface.<br>$D(\bold{m})dω_m dA$ is the total area of the portion of the corresponding microsurface whose normals lie within that specified solid angle.D需要满足下面的性质</li><li>Microfacet density is positive valued<br>$$<br>0 \leq D(\mathbf{m}) \leq \infty<br>$$</li><li>Total microsurface area is at least as large as the corresponding macrosurface’s area:<br>$$<br>1 \leq \int D(\mathbf{m}) d \omega_{m}<br>$$</li><li>The (signed) projected area of the microsurface is the same as the projected area of the macrosurface for any direction $\mathbf{v}$ :<br>$$<br>(\mathbf{v} \cdot \mathbf{n})=\int D(\mathbf{m})(\mathbf{v} \cdot \mathbf{m}) d \omega_{m}<br>$$<br>and in the special case, $\mathbf{v}=\mathbf{n}$ :<br>$$<br>1=\int D(\mathbf{m})(\mathbf{n} \cdot \mathbf{m}) d \omega_{m}<br>$$<h2 id="3-2-Shadowing-Masking-Function-G"><a href="#3-2-Shadowing-Masking-Function-G" class="headerlink" title="3.2 Shadowing-Masking Function, G"></a>3.2 Shadowing-Masking Function, G</h2></li><li>The bidirectional shadowing-masking function G(i,o,m) describes what fraction of the microsurface with normal m is visible in both directions i and o<br><img src="https://img-blog.csdnimg.cn/22361c3432504607b87594b795d9ff6a.png" alt="在这里插入图片描述">尽管它对于BSDF的形状只有relatively little effect(除了near grazing angles or for very rough surfaces),但是他需要用来保持能量守恒.他需要保持下面的性质</li><li>Shadowing-masking is a fraction between zero and one:<br>$$<br>0 \leq G(\mathbf{i}, \mathbf{o}, \mathbf{m}) \leq 1<br>$$</li><li>It is symmetric in the two visibility directions:<br>$$<br>G(\mathbf{i}, \mathbf{o}, \mathbf{m})=G(\mathbf{o}, \mathbf{i}, \mathbf{m})<br>$$</li><li>The back surface of the microsurface is never visible from directions on the front side of the macrosurface and viceversa (sidedness agreement):<br>$$<br>\begin{aligned}<br>G(\mathbf{i}, \mathbf{o}, \mathbf{m})=0 &amp; &amp; \text { if } &amp;(\mathbf{i} \cdot \mathbf{m})(\mathbf{i} \cdot \mathbf{n}) \leq 0 \<br>&amp; &amp; \text { or } &amp;(\mathbf{o} \cdot \mathbf{m})(\mathbf{o} \cdot \mathbf{n}) \leq 0<br>\end{aligned}<br>$$</li><li>确切的G函数需要微表面的细节,一般是rarely available的,但是,可以通过各种统计模型以及假设来对其进行简化.</li></ul><h2 id="3-3-Macrosurface-BSDF-Integral"><a href="#3-3-Macrosurface-BSDF-Integral" class="headerlink" title="3.3 Macrosurface BSDF Integral"></a>3.3 Macrosurface BSDF Integral</h2>]]></content>
    
    
    
    <tags>
      
      <tag>3D-reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-10-09)3D-Reconstruction Review(3)</title>
    <link href="/2023/01/10/2021-10-09-3D-Reconstruction-Review-3/"/>
    <url>/2023/01/10/2021-10-09-3D-Reconstruction-Review-3/</url>
    
    <content type="html"><![CDATA[<h1 id="Advances-in-Geometry-and-Reflectance-Acquisition-Course-Notes"><a href="#Advances-in-Geometry-and-Reflectance-Acquisition-Course-Notes" class="headerlink" title="Advances in Geometry and Reflectance Acquisition (Course Notes)"></a>Advances in Geometry and Reflectance Acquisition (Course Notes)</h1><p><strong>内容</strong>:</p><ul><li>accurate acquisition methods for geometry<br>and reflectance</li><li>efficient acquisition pipeline to fulfill the demands of industry with respect to mass digitization of 3D contents</li><li>different types of reflectance behavior ranging from diffuse over opaque to specular surfaces or even translucent and transparent surfaces</li></ul><p><strong>局限性</strong>:</p><ul><li> only well-suited for a limited range of surface materials</li><li>not adequate if no prior information with respect to the surface reflectance behavior is available</li><li>also discuss recent advances towards an efficient, fully automatic acquisition in the scope of the concluding remarks</li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>对于人眼来说:</p><ul><li>观察到的颜色和材质 not appear arbitrarily,而是由每个材料不同的反射行为导致的,这些反射行为基于材料的特征以及外表面的形状,材质特点,光照特征的共同作用.</li><li>通过对已经观察到的物体和材料的影响,可以推处further insights ,关于物理或者功能的材质</li><li>Indeed, based on visual perception, we not only get impressions about a characteristic look but also an accompanying “feel” for materials.</li></ul><p>但是会碰到很多的digital objects or materials,存在这样的问题:<br>there is often no alternative to an accurate digitization of physical objects including both geometry and reflectance characteristics where even the finest details of surface geometry and surface materials should be accurately captured.</p><h2 id="1-1-Material-Acquisition-in-Industry-and-Object-Digitization-in-Cultural-Heritage"><a href="#1-1-Material-Acquisition-in-Industry-and-Object-Digitization-in-Cultural-Heritage" class="headerlink" title="1.1 Material Acquisition in Industry and Object Digitization in Cultural Heritage"></a>1.1 Material Acquisition in Industry and Object Digitization in Cultural Heritage</h2><ul><li>电影/游戏/广告等应用中的digital materials应该transport look and feel的特征,使得其表现的像真的一样,来增加其所在场景的真实性.</li><li>In order to obtain accurate digitized surrogates of physical materials,<strong>both the surface profile and the surface reflectance behavior</strong> have to be acquired appropriately.</li><li>除了上述的应用之外,还有很多应用需要3D的形状和反射行为,比如 digital preservation of objects for cultural heritage,比如通过 highly optimized devices as discussed in:[].</li><li>虽然geometry常以点云等形式存储,但是反射率以n terms of different reflectance functions that describe the material appearance depending on a multitude of involved parameters进行存储<h2 id="1-2-The-Acquisition-Ecosystem"><a href="#1-2-The-Acquisition-Ecosystem" class="headerlink" title="1.2 The Acquisition Ecosystem"></a>1.2 The Acquisition Ecosystem</h2><img src="https://img-blog.csdnimg.cn/fb8c85ddc58a4b2ca99c204551d3b686.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_13,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li><strong>Hardware</strong>: required for the acquisition.包括各种sensor,照明设备,用来build a gantry或者搭建其他的设备的部件.</li><li><strong>Calibration</strong>:techniques for the radiometric and geometric calibration of the involved components.通过Calibration techniques可以在多个view以及多个光照条件下观察物体.这需要考虑组件的arrangement以及sensor和照明设备的特征.</li><li><strong>geometry acquisition techniques</strong>:为了可信的重建,该技术需要被用来得到geometry足够的细节.各种文献中已证明,不存在通用的处理可能由所有material组成的物体的技术,任意技术都局限在a limited fraction of the materials.</li><li><strong>reflectance acquisition techniques</strong>:一样是a limited fraction of the materials.由于其复杂性导致了a number of parameters are resolved and measured.各种发射光照模型需要different acquisition devices进行配合.由于measure的参数越多,需要的时间就越长,所以人们选择合适的,只得到需要参数的模型.</li><li><strong>visualization</strong>:visualization techniques to depict the digitized models.</li><li><strong>assistance systems</strong>:increase the efficiency,such as e.g. a prior material recognition step</li></ul><h2 id="1-3-Course-Objectives"><a href="#1-3-Course-Objectives" class="headerlink" title="1.3  Course Objectives"></a>1.3  Course Objectives</h2><ul><li> accurately capturing the 3D shape of objects and optical properties of materials is especially challenging for objects made of materials with complex reflectance behavior</li><li>没有通用的方法,user typically selects appropriate ones based on his experience<br><img src="https://img-blog.csdnimg.cn/621edd2c96694a7693d8c83f4cbb5226.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>对创建大量数字内容的需求也将重点转向automatic and highly efficient solutions.由于这里可能没有先验知识,the presence of the individual, occurring surface materials should guide the acquisition process.</li><li>instead of naively processing(上面图片的过程中),只有那种物体表面部分的反射行为可方法对应时,才选择他们,这样更加高效.而且,based on the material properties, there is also the possibility to automatically detect cases where none of the available reconstruction techniques is appropriate.从而在actual acquisition之前,这里需要有个可信的对present surface materials.的识别,然后利用该信息取指导acquision的过程.<br><img src="https://img-blog.csdnimg.cn/6f59bc98039e48c8947dfca41e01faa4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li><strong>这篇survey主要将三个方面的问题</strong><br>  1.advances in geometry acquisition(section 4),<br>  2.advances in reflectance acquisition(section 5),<br>  \3. concepts towards a more efficient automatic acquisition process(section 6)</li></ul><h1 id="2-Preliminaries-of-Material-Appearance"><a href="#2-Preliminaries-of-Material-Appearance" class="headerlink" title="2.Preliminaries of Material Appearance"></a>2.Preliminaries of Material Appearance</h1><ul><li>分析mmaterial apperance,对material的性质以及他们如何能够从image content推导出来的研究是很重要的</li><li><strong>key observation</strong>:the visual complexity of surface appearance is characterized by the complex interplay of surface material, surface geometry and illumination.</li><li> <strong>a chicken-and-egg problem</strong>:standard acquisition devices值能够捕获the coupling of the respective modalities,而又需要将这些模式分开进行分析,而这又需要每个模式相关的priori information.</li><li><strong>Section 2.1</strong>: the characteristics of material appearance and discuss the dependency of material appearance with respect to scale, illumination and surface geometry.</li><li><strong>Section 2.2</strong>:从与反射性质导出的光线传输特征进行的a taxonomy of surface classes(relevant for 3D geometry acquisition)</li><li><strong>Section 2.3</strong>: an overview of commonly used reflectance models<h2 id="2-1-Basics-of-Material-Appearance"><a href="#2-1-Basics-of-Material-Appearance" class="headerlink" title="2.1 Basics of Material Appearance"></a>2.1 Basics of Material Appearance</h2>We first may have a closer look at the underlying physical effects that characterize material appearance.Section 2.1.1中回顾radiometry 然后咋2.1.2中讨论 the physical background with respect to light exchange at material surfaces<h3 id="2-1-1-Radiometric-Quantities"><a href="#2-1-1-Radiometric-Quantities" class="headerlink" title="2.1.1 Radiometric Quantities"></a>2.1.1 Radiometric Quantities</h3></li><li><strong>ray optics</strong>:由于光线直线传播的特点,光通常被表示成straight lines,并称之为 ray representation<br>A ray $\mathbf{r}$ can be parameterized as a mapping $\mathbb{R}^{+} \rightarrow \mathbb{R}^{3}$ using the ray origin $\mathbf{o} \in \mathbb{R}^{3}$ and a ray direction $\mathbf{d}=\left(d_{1}, d_{2}, d_{3}\right)^{T}$<br>$$<br>\begin{aligned}<br>\mathbf{r}: \mathbb{R}^{+} &amp; \rightarrow \mathbb{R}^{3} \<br>s &amp; \mapsto \mathbf{o}+s \mathbf{d}<br>\end{aligned}<br>$$</li><li>对于polychromatic光(多色光,单色光称为monochromatic),常被表示为a spectral power distribution $\lambda\in R^+\to L(\lambda)\in R^+$,其中L是 electromagnetic radiative power.这些波长的框分为 ultraviolet (UV) spectrum,infrared (IR) spectrum以及可见光wavelengths λ between 380nm (violet) and 780 nm (red).本课程中,光一般用<strong>tristimulus values</strong> R/G/B 表示,<strong>他们的值可以通过</strong>derived from a spectral power distribution based on inner products with suitable color matching functions such as e.g. CIE RGB [Smith and Guild 1931].得到</li><li><strong>radiant energy</strong>:Q[J],</li></ul><p><strong>the radiant flux or radiant power</strong>:$\Phi[J s^{-1}]=\frac{dQ}{dt}$<br>(flux中没有表示方向,但是光的反向是存在的,比如glossy or<br>specular materials)<br><strong>irradiance</strong>:$E=\frac{d\Phi}{dA}[Wm^{-2}]$( arriving at the surface)<br><strong>exitance</strong>:$E=\frac{d\Phi}{dA}[Wm^{-2}]$( leaving the surface)<br>对于surfaces with inhomogeneous flux,the irradiance or exitance depends on the local surface point x,$E=E(x)$<br>对于surfaces with homogeneous flux,the irradiance can be represented by the total flux per surface area<br><strong>intensity</strong>:$I=\frac{d\Phi}{d\omega}[W~sr^{-1}]$(The unit of the solid angle is Steradian [sr])<br>It defines the flux per differential solid angle.<br><strong>radiance</strong>:$L=\frac{d\Phi}{dAd\omega}[Wm^{-2}sr^{-1}]$(这里的A是projected differential area dA,沿着light flow的direction),所以又有公式<br>$L(\omega)=\frac{d^2\Phi}{dA_{\perp}d\omega}=\frac{d^2\Phi}{dAcos\theta d\omega}$,其中inclination angle θ is defined between the local surface normal n and the direction of the light flow<br><strong>重要公式</strong>:到达某个面元的irradiance可以通过在该表面的upper hemisphere Ω上进行积分得到<br>$$<br>E=\frac{\mathrm{d} \Phi}{\mathrm{d} A}=\int_{\Omega} L \cos (\theta) \mathrm{d} \omega<br>$$<br>可以发现,这里的$\theta$和$\omega$是相关的.</p><ul><li>在真空的假设下,, the radiance remains constant along a ray [Glassner 1995].而对于medium的情况,光线传播可以建模为interaction events that change the power of the light ray.</li><li><strong>Polarization</strong>:describes the orientation of the electromagnetic wave perpendicular to the propagation direction in space.在识别技术中,偏振现象优势被用来 employing polarization filters to <strong>separate the direct and the global components</strong></li></ul><h3 id="2-1-2-Light-Interaction-at-Surfaces"><a href="#2-1-2-Light-Interaction-at-Surfaces" class="headerlink" title="2.1.2 Light Interaction at Surfaces"></a>2.1.2 Light Interaction at Surfaces</h3><p><img src="https://img-blog.csdnimg.cn/37a71529e2cf47fa8be387327a2cfc2d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>这里总共可能有12维(因为$x_i,x_r$在曲面上,所以他们分别是二维的)</li><li>一般来说可以假设:<strong>“</strong> Typical assumptions made in the great majority of publications are that the light transport at the surface happens in an infinitesimal period (i.e. $t_i = t_r$), that there is no time dependency of the reflectance behavior (i.e.$t_0 = t_i = t_r$), that the wavelength remains unchanged (i.e. $λ_i = λ_r$) and that the incoming flux is completely reflected at the surface (i.e. $x_i = x_r$).<strong>“</strong></li><li>考虑时需要分scale进行考虑”take into account that material appearance is a scale-dependent phenomenon”,分别是microscopic scale,a slightly coarser scale(相对于前面那个),mesoscopic scale,macroscopic scale.</li><li>microscopic scale:domain of quantum optics,这些结构不会被眼睛观测,但是对材料外观有极大的贡献,determine the appearance of all materials.</li><li>a slightly coarser scale:domain of wave optics(考虑光线和与波长近似的小结构interaction的现象,比如diffraction与polarization)</li><li>mesoscopic scale:光线与表面上的fine details的相互作用,比如scratches,engraving,weave-patterns od textiles和embossing of leathers.这些表面的结构可能会导致 self-shadowing, self-occlusions,or interreflections <img src="https://img-blog.csdnimg.cn/19415e2bceff4a88a2612b54189c46f6.png" alt="在这里插入图片描述"></li><li>macroscopic scale:3D geometry对于一些材质仍然影响其apperance.woven cloth, brushed metal等可能在照片中会distorted,因为其对物体geometry的依赖.</li><li>但是上述几种scale只对近距离观察起作用.对于增加的距离,在fine surface detail上的the effect of light exchange会变得不可见.所以他们会被当成irregularities in a different kind of microscopic scale.例子:”shininess of specular objects or translucency might also depend on the distance between object and observer.”</li><li>当考虑一个a highly specular surface with a rough surface profile from a close range时,因为人眼的分辨率足够将其分为不同的patch并得到其法向,所以这个材料显得specular.但是当距离增加时,只能够观察到superposition of these pathces.这样便会导致从specular到diffuse的过渡.然而,当考虑一个highly specular surfaces时,这个表面随着距离的增加仍然是specular的.</li><li>这些现象表明:<strong>The definitinon of scale is of dynamic nature</strong>.<h2 id="2-2-A-Taxonomy-of-Surface-Classes"><a href="#2-2-A-Taxonomy-of-Surface-Classes" class="headerlink" title="2.2 A Taxonomy of Surface Classes"></a>2.2 A Taxonomy of Surface Classes</h2>前面已经提到,没有一种acquisition technique可以处理任意的材料,需要根据the acquisition principles applicable to the individual material groups 来分类材料.这些分组是与complexity of their visual appearance有关的.<br>这里只考虑固体的分类,不考虑volumetric phenomena(fog or fire).这里的分类是根据下面的survey进行的:<br>IHRKE, I., KUTULAKOS, K. N., LENSCH, H. P. A., MAGNOR, M., AND HEIDRICH, W. 2010. Transparent and specular object reconstruction. Computer Graphics Forum 29, 8, 2400–2426.<br><img src="https://img-blog.csdnimg.cn/18cf65eb2b114917976606df67700f47.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-2-1-Rough-Surfaces-with-Diffuse-or-Near-Diffuse-Reflectance"><a href="#2-2-1-Rough-Surfaces-with-Diffuse-or-Near-Diffuse-Reflectance" class="headerlink" title="2.2.1 Rough Surfaces with Diffuse or Near Diffuse Reflectance"></a>2.2.1 Rough Surfaces with Diffuse or Near Diffuse Reflectance</h3>需要注意的是,入射光和反射光这里有两个和法向相关的角,在diffuse的情形中,是如入射角有关,与反射角无关的.<br>$$<br>L_{diffuse}=L_ik_{diffuse}cos\theta,\<br>cos θ = n d_i<br>$$<br>这里k_{diffuse}是衡量表面diffusivity的常量.<h3 id="2-2-2-Glossy-Surfaces-with-Mixed-Diffuse-and-Specular-Reflectance"><a href="#2-2-2-Glossy-Surfaces-with-Mixed-Diffuse-and-Specular-Reflectance" class="headerlink" title="2.2.2  Glossy Surfaces with Mixed Diffuse and Specular Reflectance"></a>2.2.2  Glossy Surfaces with Mixed Diffuse and Specular Reflectance</h3>$$<br>L_{glossy}=L_{diffuse}+L_{specular}<br>$$<br>more light is reflected into preferred directions and the observed material appearance is view-dependent<h3 id="2-2-3-Smooth-Surfaces-with-Ideal-or-Near-Ideal-Specular-Reflectance"><a href="#2-2-3-Smooth-Surfaces-with-Ideal-or-Near-Ideal-Specular-Reflectance" class="headerlink" title="2.2.3 Smooth Surfaces with Ideal or Near Ideal Specular Reflectance"></a>2.2.3 Smooth Surfaces with Ideal or Near Ideal Specular Reflectance</h3>(almost) completely reflected into the direction:<br>$$<br>d_{o,ideal~reflection}=2n(nd_i)-d_i<br>$$<br>由于这种情况下,镜面反射的特征没有自己的characteristic appearance 而是反映的是surrounding environment in a view-dependent manner.所以the geometry reconstruction for objects with such a reflectance behavior is rather challenging<h3 id="2-2-4-Surfaces-Where-Light-is-Scattered-Multiple-Times-Underneath-the-Surface"><a href="#2-2-4-Surfaces-Where-Light-is-Scattered-Multiple-Times-Underneath-the-Surface" class="headerlink" title="2.2.4  Surfaces Where Light is Scattered Multiple Times Underneath the Surface"></a>2.2.4  Surfaces Where Light is Scattered Multiple Times Underneath the Surface</h3>这种现象在 translucent object中较多,由于光纤transport within the object导致的.一些光线进入物质进行scatter,然后意味着一部分的光线离开物质的位置和入射点不同.这将会导致a blurring of the observed pattern.这种现象 make a triangulation-based reconstruction from the decoded correspondences rather unreliable.<h3 id="2-2-5-Smooth-Surfaces-with-Ideal-or-Near-Ideal-Specular-Refraction"><a href="#2-2-5-Smooth-Surfaces-with-Ideal-or-Near-Ideal-Specular-Refraction" class="headerlink" title="2.2.5 Smooth Surfaces with Ideal or Near Ideal Specular Refraction"></a>2.2.5 Smooth Surfaces with Ideal or Near Ideal Specular Refraction</h3>Snell’s law:$\eta_{1} \sin \theta_{1}=\eta_{2} \sin \theta_{2}$<br>这里存在的问题是,objects might also exhibit inhomogeneous reflectance characteristics(树立在水中的木杆在交界处有一个kink),by a spatially varying refractive index or by inclusions of Lambertian or opaque material components as given in many minerals.<br><img src="https://img-blog.csdnimg.cn/8b653c1a7deb4b97817e3e2e509ada9d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><h2 id="2-3-常用的反射模型"><a href="#2-3-常用的反射模型" class="headerlink" title="2.3 常用的反射模型"></a>2.3 常用的反射模型</h2><p>摘要:论文中经常出现的<strong>SVBRDF,还有6D,4D模型</strong>怎么来的</p><ul><li>基于前面的分类,可以知道:diffuse and specular components以及可能发生的材料下的scattering以及refraction需要考虑进反射模型中,同时各种reflectance acquisition需要在相应的反射模型假设下进行设计.各种不同的模型针对于部分可能的材料,而且希望在尽可能少的参数下对材料在acceptable的时间中进行一个可信描绘.</li><li>对于diffuse的物质,更多的考虑different material characteristics而不是modeling surface reflectance of mirrors,类似的,考虑同时有diffuse and specular或者translucent 和transparent的物质,considering the respectively relevant characteristics of the individual materials更加重要.即<strong>reflectance acquisition strongly depends on the representation used to model the reflectance of a particular material</strong></li><li>很通用的表示:$\rho\left(\mathbf{x}<em>{i}, \theta</em>{i}, \varphi_{i}, t_{i}, \lambda_{i}, \mathbf{x}<em>{r}, \theta</em>{r}, \varphi_{r}, t_{r}, \lambda_{r}\right)$,这里$t_i=t_r,\lambda_i=\lambda_r$一般成立.<br><img src="https://img-blog.csdnimg.cn/9ab9e3238c4f4aaabb90afc282f866af.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><p>详细称呼:</p><ul><li>BSSRDF:bidirectional scattering-surface reflectance distribution function</li><li>SVBRDF:spatially-varying bidirectional reflectance distribution function</li><li>BTF:bidirectional texture function</li><li>BSSDF:bidirectional subsurface scattering distribution function</li><li>BRDF: the bidirectional reflectance distribution function</li><li>SLF:surface light field</li><li>SRF:surface reflectance field</li></ul><p>一行一行的来理解:</p><ul><li>8D模型要求的是反射场定义在一个凸的表面删,然后viewpoint以及light都来自于bounding volume.这样可以通过base incideng light field的线性表达式来定义新的光照条件,</li><li>8D到SVBRDF是假设没有内部的scatter,从而入射和反射点是一致的.而到BTF是假设各处入射光照都是一致的(light source infinitely far away),即$\rho_{L F, i}\left(\mathbf{x}<em>{i}, \theta</em>{i}, \varphi_{i}\right)=\rho_{L F, i}\left(\theta_{i}, \varphi_{i}\right) .$,所以这里SVBRDF和BTF看起来形式差不多,但是意义上有差别.<strong>n comparison to SVBRDFs, BTFs allow to capture local subsurface scattering characteristics as well as mesoscopic effects such as interreflections, self-masking or self-occlusions</strong>.当考虑各向同性的(homogeneous reflectance)是,8D模型由编程了BSSDF模型$\rho_{\mathrm{BSSDF}}\left(\theta_{i}, \varphi_{i}, \mathbf{x}<em>{r}-\mathbf{x}</em>{i}, \theta_{r}, \varphi_{r}\right)$</li><li>在BSSRDF的基础上,再要求non-subsurface scattering reflectanc,得到BRDF.在BTF的基础上,固定lighting的方向得到SLF,固定view的方向得到SRF.</li><li>对于2D模型来说,它是建立在diffuse reflectance的基础上的,得到texture maps of bump maps.</li></ul><p>上面的层次图来源于<br>M ¨ULLER, G., MESETH, J., SATTLER, M., SARLETTE, R., AND KLEIN, R. 2004. Acquisition, synthesis and rendering of bidirectional<br>texture functions. In Eurographics 2004 State of the Art Reports, 69–94.<br>后面在section 5还会继续讨论 individual reflectance models and their acquisition的细节.<br><strong>关于上面每个模型详细的论文见:p13</strong></p><h1 id="3-Calibration"><a href="#3-Calibration" class="headerlink" title="3.Calibration"></a>3.Calibration</h1><ul><li>为什么要讲Calibration?:<strong>If several measurements of the geometry in a scene or the corresponding reflectance behavior have been performed,these measurements need to be brought into some kind of reference system.</strong></li><li>在这里仅仅为了<strong>establish an awareness regarding why individual calibration techniques are needed and some rather general ideas on how a calibration of individual setup components can be performed</strong>,对于细节来说,由于参考具体文献.</li><li>由于individual components相对于物体表面的位置/方向以及device-specific 的特征对于geometry和reflectance的an accurate acquisition是很重要的.从而<strong>geometric calibration</strong>是很重要的,它可以得到 the involved illuminants, imaging sensors and the object surface during the acquisition之间的联系.这里还包括在投影中的相机的focal length以及principle point.<strong>radiometric calibration</strong>也是很重要的,它考虑了sensor以及illuminants的radiometric性质.这在将测得的反射性质用于 some kind of reference system for radiance中很重要.<h2 id="3-1-Geometric-Calibration"><a href="#3-1-Geometric-Calibration" class="headerlink" title="3.1 Geometric Calibration"></a>3.1 Geometric Calibration</h2></li><li>The geometric calibration aims at the specification of <strong>relative positions and orientations</strong> of the individual components involved in the setup by either using <strong>certain known calibration objects or possibly also objects with unknown shape</strong></li><li>对于这些物体的不同的观测需要进行register,同时需要考虑光源的位置.同时通常没有足够的sensor来提供足够稠密的view-light configurations.所以需要通过turntable来模拟很多sensor的效果.<h3 id="3-1-1-Camera-Calibration"><a href="#3-1-1-Camera-Calibration" class="headerlink" title="3.1.1 Camera Calibration"></a>3.1.1 Camera Calibration</h3>详细介绍见:HARTLEY, R. I., AND ZISSERMAN, A. 2004. Multiple view geometry in computer vision (2nd edition). Cambridge University Press,<br>Cambridge, UK<br>由于这一部分较为基础,不做记录,对应页号为P15</li></ul><h3 id="3-1-2-Light-Source-Calibration"><a href="#3-1-2-Light-Source-Calibration" class="headerlink" title="3.1.2 Light Source Calibration"></a>3.1.2 Light Source Calibration</h3><ul><li>有些时候,虽然light source是放在一个机器臂上的,他们的位置可以直接得到,但是”positioning systems often do not offer the possibility for a precise specification of a light source position and orientation”,所以Calibration是必须的.</li><li>下面的文章中用到的方法是基于一致位置以及半径的mirroring calibration sphere上的highlight observations.<br>(1) CHEN, T., GOESELE, M., AND SEIDEL, H.-P. 2006. Mesostructure from specularity. In Proceedings of the IEEE Conference on Computer<br>Vision and Pattern Recognition (CVPR), vol. 2, 1825–1832.<br>(2)WEINMANN, M., RUITERS, R., OSEP, A., SCHWARTZ, C., AND KLEIN, R. 2012. Fusing structured light consistency and Helmholtz<br>normals for 3d reconstruction. In Proceedings of the British Machine Vision Conference (BMVC), 1–12.<br>(3)SCHWARTZ, C., SARLETTE, R., WEINMANN, M., AND KLEIN, R. 2013. DOME II: A parallelized BTF acquisition system. In Proceedings<br>of the Eurographics Workshop on Material Appearance Modeling: Issues and Acquisition, 25–31.</li><li>其关键想法是追踪从calibrated camera的perspective center出发的ray,穿过图片上highlight的位置,到达scene中,这里他们被一致的shpere geometry 反射.”Reflected rays obtained from highlights observed from multiple spheres allow the reconstruction of the light source position which is given by their intersection”<br><img src="https://img-blog.csdnimg.cn/aabed40d0cfa44c78884ce37573d946d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>而在这篇论文中:SCHWARTZ, C., SARLETTE, R., WEINMANN, M., RUMP, M., AND KLEIN, R. 2014. Design and implementation of practical bidirectional<br>texture function measurement devices focusing on the developments at the University of Bonn. Sensors 14, 5, 7753–7819.<br>使用来一种同时对光源位置以及calibration sphere的位置的非线性优化算法,来减少观察到的反射中的 re-projection error.</li><li>在ACKERMANN, J., FUHRMANN, S., AND GOESELE, M. 2013. Geometric point light source calibration. In Proceedings of Vision, Modeling<br>and Visualization (VMV), 161–168.的论文中,与上面的方法相似,只是在优化的过程中没有refine calibration sphere.</li><li>而在这两篇论文中,M ¨ULLER, G., MESETH, J., SATTLER, M., SARLETTE, R., AND KLEIN, R. 2004. Acquisition, synthesis and rendering of bidirectional<br>texture functions. In Eurographics 2004 State of the Art Reports, 69–94.<br>SCHWARTZ, C., WEINMANN, M., RUITERS, R., AND KLEIN, R. 2011. Integrated high-quality acquisition of geometry and appearance<br>for cultural heritage. In Proceedings of the International Symposium on Virtual Reality, Archaeology and Intelligent Cultural Heritage<br>(VAST), 25–32.<br>如果camera 的闪光也作为光源,那么光源的位置可以基于闪光相对于相机透视中心的偏移量进行估计<h3 id="3-1-3-Turntable-Calibration"><a href="#3-1-3-Turntable-Calibration" class="headerlink" title="3.1.3 Turntable Calibration"></a>3.1.3 Turntable Calibration</h3></li><li>Turntable在大多数的setup中用来模拟更远处light source或者image sensor的存在.</li><li>相当精准的对于turntable center和turntable axis的估计可以通过rotating calibration targets达到.这些target被相机观察到(相机也可以通过这些target进行标定),然后对应的观察可以用来得到turntable的参数.一些高质量的turntable可以直接提供足够精确度的rotation angle.<h2 id="3-2-Radiometric-Calibration"><a href="#3-2-Radiometric-Calibration" class="headerlink" title="3.2 Radiometric Calibration"></a>3.2 Radiometric Calibration</h2></li><li>除了几何calibration之外,在使用像素值进行geometry以及reflectance的重建时,光源发出的光线的性质以及传感器将接收到的光转换为特定的像素值的特征也需要进行考虑.</li><li>…<h1 id="4-Advances-in-Geometry-Acquisition"><a href="#4-Advances-in-Geometry-Acquisition" class="headerlink" title="4. Advances in Geometry Acquisition"></a>4. Advances in Geometry Acquisition</h1>尽管有很多几何重建的方法,但是经证明,许多传统的技巧不允许重建objects with arbitrary surface reflectance的geometry,而只能重建objects with a rather simple diffuse surface reflectance behavior.这也就导致来大量的diverse material-specific acquisition techniques,每个技巧只能针对于特定范围的材料.很多方法的材料分类根据前面Section 2的分类方法.<br>在介绍trend之前,首先我们在4.1节介绍an overview of standard principles (Section 4.1) that are used in many of the geometry acquisition techniques.<h2 id="4-1-Basic-Geometry-Acquisition-Principles"><a href="#4-1-Basic-Geometry-Acquisition-Principles" class="headerlink" title="4.1 Basic Geometry Acquisition Principles"></a>4.1 Basic Geometry Acquisition Principles</h2>The acquisition of the 3D surface geometry of objects can be approached based on different principles. The key differences of the individual methods include whether methods are <strong>active or passive, contact-based or non-contact-based and optical or non-optical</strong><br><img src="https://img-blog.csdnimg.cn/a5f343e6a15745cba11291f2078da9a7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/04c9c4a6653e4f10bd10143e17e7de81.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><p>Active methods就是可以向物体打a certain type of energy,并且可以被the components of the respective acquisition setup or in terms of contact-based feelers that produce a characteristic signal when touching the surface观测到,而passive的方法是只有fixed and distant illumination,并且关注对这种照明条件下optical object appearance的特征.</p><h3 id="4-1-1-Principles-for-Passive-Geometry-Acquisition"><a href="#4-1-1-Principles-for-Passive-Geometry-Acquisition" class="headerlink" title="4.1.1 Principles for Passive Geometry Acquisition"></a>4.1.1 Principles for Passive Geometry Acquisition</h3><p><strong>observation and analysis of the optical appearance of objects under fixed illumination without systematic manipulations.</strong></p><h4 id="4-1-1-1-Shape-from-Shading-Techniques"><a href="#4-1-1-1-Shape-from-Shading-Techniques" class="headerlink" title="4.1.1.1 Shape-from-Shading Techniques"></a>4.1.1.1 Shape-from-Shading Techniques</h4><ul><li>image上面得到的一个view的2D的信息是不足以重建3D的物体形状的,这是由于该问题不适定性的本质导致的(ill-posed),为了能够重建表面的geometry,需要一些prior information或者assumption来得到额外的约束<br>(经常看到提远近场光的:<a href="https://baike.baidu.com/item/%E8%BF%91%E5%9C%BA%E5%85%89%E5%AD%A6/11045876?fr=aladdin">https://baike.baidu.com/item/%E8%BF%91%E5%9C%BA%E5%85%89%E5%AD%A6/11045876?fr=aladdin</a>)</li><li>传统的shape-from-shading techniques基于分析单一的view观察到的,在可控的环境下可控的点光源光照下,观察到的intensity information,而illumination characteristics,比如波长以及它的origin(一般是远场光,the light source position is typically significantly farer away than the extents of the considered object surfaces)和方向由prior calibration procedure已知.但是现在的问题仍然是ill-posed.</li><li>而对于diffuse surface的Lambert’s law,可以添加更多的约束:<br>$$<br>I(x,y=\alpha(x,y) r l \cdot n<br>$$<br>这里$I(x,y)$是2D图像上观察的的intensity,l是已知的incoming illumination,$\alpha(x,y)$是一致的表面的albedo.r是known intensity r of the illumination</li><li>额外的广泛使用的假设包括 an orthographic image projection and the absence of shadows and interreflections(正射,没有阴影以及多次反射).然后现在的方程组对于一个像素只有一个约束,所以仍然是<strong>under-constrained</strong>的,这里对于每一个表面上的反向有两个未知的参数$p=\frac{dz}{dx}$以及$p=\frac{dz}{dy}$.因为这个原因,需要更近一步的假设,这个假设,假设$\alpha(x,y)$为常数,并且对法向$n$中的z分量进行normalization得到$n=[-p,-q,1]^T$,并且限制梯度(比如enforcing smoothness),或者使用the self-shadow boundary进行限制.</li><li>针对于shape-from-shading更加详细的讨论见DUROU, J.-D., FALCONE, M., AND SAGONA, M. 2008. Numerical methods for shape-from-shading: A new survey with benchmarks.<br>Computer Vision and Image Understanding 109, 1, 22–43.</li><li>在surface normal的基础上,表面的geometry可以通过法向场积分的技巧得到(normal field integration techniques.).大多数的shape-from-shading techniques都聚焦于单张图片的几何重建,因此<strong>only a 2.5D height map can be derived</strong></li></ul><h4 id="4-1-1-2-Shape-from-Texture-Techniques"><a href="#4-1-1-2-Shape-from-Texture-Techniques" class="headerlink" title="4.1.1.2 Shape-from-Texture Techniques"></a>4.1.1.2 Shape-from-Texture Techniques</h4><ul><li>一些物质表现出了相当regular的表面样式可能可以用来surface reconstruction,比如fabrics(regular structure是由编织的样式决定的).Shape-from-Texture Techniques关注假定有regular statistical ot geometric surface pattern的物品的表面几何重建</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>3D-reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-10-07)3D-Reconstruction Review(2)</title>
    <link href="/2023/01/10/2021-10-07-3D-Reconstruction-Review-2/"/>
    <url>/2023/01/10/2021-10-07-3D-Reconstruction-Review-2/</url>
    
    <content type="html"><![CDATA[<h1 id="文献阅读"><a href="#文献阅读" class="headerlink" title="文献阅读"></a>文献阅读</h1><h2 id="Learning-Efficient-Illumination-Multiplexing-for-Joint-Capture-of-Reflectance-and-Shape"><a href="#Learning-Efficient-Illumination-Multiplexing-for-Joint-Capture-of-Reflectance-and-Shape" class="headerlink" title="Learning Efficient Illumination Multiplexing for Joint Capture of Reflectance and Shape"></a>Learning Efficient Illumination Multiplexing for Joint Capture of Reflectance and Shape</h2><h3 id="一-问题背景"><a href="#一-问题背景" class="headerlink" title="一. 问题背景"></a>一. 问题背景</h3><h4 id="1-问题介绍"><a href="#1-问题介绍" class="headerlink" title="1.问题介绍"></a>1.问题介绍</h4><ul><li>joint acquisition of unknown reflectance and shape</li><li>efficient capture of both reflectance and shape is fundamentally challenging<br>(1)渲染方程的高维性以及与image-based measurement的紧密联系.所以需要 take as many measurements as possible来得到足够的信息<br>James T. Kajiya. 1986. The Rendering Equation (SIGGRAPH ’86). 143–150.<br>(2)实际应用中,the number of samples can be<br>strictly limited.而其解决方法是优化acquisition efficiency</li></ul><h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h4><h5 id="2-1-综述"><a href="#2-1-综述" class="headerlink" title="2.1 综述"></a>2.1 综述</h5><p>样品反射率和形状的重建可以分为两个种类,分类是基于入射光是否被控制.在相关工作中仅介绍被控制的情形.可参考下面的综述.</p><ul><li>Yue Dong. 2019. Deep appearance modeling: A survey. Visual Informatics (2019).</li><li>Darya Guarnera, Giuseppe C. Guarnera, Abhijeet Ghosh, Cornelia Denk, and Mashhuda Glencross. 2016. BRDF Representation and Acquisition. Computer Graphics Forum 35, 2 (2016), 625–650</li><li>Michael Weinmann and Reinhard Klein. 2015. Advances in Geometry and Reflectance Acquisition. In SIGGRAPH Asia Courses. Article 1, 71 pages.</li><li>Tim Weyrich, Jason Lawrence, Hendrik P. A. Lensch, Szymon Rusinkiewicz, and Todd Zickler. 2009. Principles of Appearance Acquisition and Representation. Found.Trends. Comput. Graph. Vis. 4, 2 (2009), 75–191.<h5 id="2-2-Geometry-Reconstruction-with-the-Diffuse-Assumption"><a href="#2-2-Geometry-Reconstruction-with-the-Diffuse-Assumption" class="headerlink" title="2.2 Geometry Reconstruction with the Diffuse Assumption"></a>2.2 Geometry Reconstruction with the Diffuse Assumption</h5></li><li>下面的方法是基于diffuse-dominant reflectance假设(它是多个view 下的不变量,用来在multi-view之间建立对应),但是对于被general SVBRDF表达的外观就不行了:<em>“</em> the reflectance that changes with the view is often treated as outliers, or physically modified via means like powder coating.<em>“</em><br>(1)Daniel Scharstein and Richard Szeliski. 2003. High-accuracy stereo depth maps using structured light. In CVPR.<br>高质量的几何重建可以基于active illumination方法,比如strutured lighting<br>(2)Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm.2016. Pixelwise View Selection for Unstructured Multi-View Stereo. In ECCV.<br>对于丰富表面纹理的样品,passive的方法,比如structure-from-motion也可以很好的恢复形状</li><li>Robert J Woodham. 1980. Photometric method for determining surface orientation from multiple images. Optical engineering 19, 1 (1980), 191139.<br>另一条线是 photometric stereo.从不同光照条件下外观的变化,它估计法向场,而法向场可以合并到一个3D的表面上去.<br>这一种方法,最近的研究仍然局限于各向同性的镜面反射光,并需要 96 distant lighting conditions<br>Satoshi Ikehata. 2018. CNN-PS: CNN-based photometric stereo for general non-convex surfaces. In ECCV<h5 id="2-3-Spatially-Varying-Reflectance-Capture-on-a-Known-Shape"><a href="#2-3-Spatially-Varying-Reflectance-Capture-on-a-Known-Shape" class="headerlink" title="2.3  Spatially-Varying Reflectance Capture on a Known Shape"></a>2.3  Spatially-Varying Reflectance Capture on a Known Shape</h5></li><li>对6D定义域的SVBRDF的直接采样,通过mechanically positioning a camera and a light source是及其费时的<br>Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. 1999.<br>Reflectance and Texture of Real-world Surfaces. ACM Trans. Graph. 18, 1 (Jan. 1999),<br>1–34.<br>Jason Lawrence, Aner Ben-Artzi, Christopher DeCoro, Wojciech Matusik, Hanspeter<br>Pfister, Ravi Ramamoorthi, and Szymon Rusinkiewicz. 2006. Inverse Shade Trees<br>for Non-parametric Material Representation and Editing. ACM Trans. Graph. 25, 3<br>(July 2006), 735–745.<br>对于反射率数据的priors被引入减少消耗,包括基本材料的线性组合:<br>Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter<br>Seidel. 2003. Image-based Reconstruction of Spatial Appearance and Geometric<br>Detail. ACM Trans. Graph. 22, 2 (April 2003), 234–257<br>Hongzhi Wu, Zhaotian Wang, and Kun Zhou. 2016. Simultaneous Localization and<br>Appearance Estimation with a Consumer RGB-D Camera. IEEE TVCG 22, 8 (Aug</li></ul><p>2016), 2012–2023.<br>以及反射率位于低维流形上:<br>Yue Dong, Jiaping Wang, Xin Tong, John Snyder, Yanxiang Lan, Moshe Ben-Ezra, and<br>Baining Guo. 2010. Manifold Bootstrapping for SVBRDF Capture. ACM Trans.<br>Graph. 29, 4, Article 98 (July 2010), 10 pages.<br>以及stochastic-texture-like的材料的假设<br>Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. 2015. Two-shot SVBRDF Capture for<br>Stationary Materials. ACM Trans. Graph. 34, 4, Article 110 (July 2015), 13 pages.</p><ul><li>Illumination-multiplexing-based approaches可以有效的获取高质量的结果,大量的光宇被同时program<br>(1)Abhijeet Ghosh, Tongbo Chen, Pieter Peers, Cyrus A. Wilson, and Paul Debevec. 2009.<br>Estimating Specular Roughness and Anisotropy from Second Order Spherical Gradient Illumination. Computer Graphics Forum 28, 4 (2009), 1161–1170.<br>通过捕捉spherical harmonics lighting patterns下的照片,然后通过人工推导的反向查询表(which maps the observed radiance to BRDF parameters)来恢复反射率<br>(2)Guojun Chen, Yue Dong, Pieter Peers, Jiawan Zhang, and Xin Tong. 2014. Reflectance<br>Scanning: Estimating Shading Frame and BRDF with Generalized Linear Light<br>Sources. ACM Trans. Graph. 33, 4, Article 117 (July 2014), 11 pages.<br>Andrew Gardner, Chris Tchou, Tim Hawkins, and Paul Debevec. 2003. Linear light<br>source reflectometry. ACM Trans. Graph. 22, 3 (2003), 749–758.<br>线性光源在平面材料的样品上移动,并通过对应的外观变量对SVBRDF进行重建<br>(3)Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. 2013. Practical SVBRDF Capture in<br>the Frequency Domain. ACM Trans. Graph. 32, 4, Article 110 (July 2013), 12 pages.<br>使用相机和近场的LCD板作为光源,基于频域分析,来获得各向同性的反射率</li><li>Kaizhang Kang, Zimin Chen, Jiaping Wang, Kun Zhou, and Hongzhi Wu. 2018. Efficient<br>Reflectance Capture Using an Autoencoder. ACM Trans. Graph. 37, 4, Article 127<br>(July 2018), 10 pages.<br>就是上一篇阅读的文章.那一篇文章不能直接向这里推广的原因是 <strong>“</strong> the extra complexity of unknown geometry, and the complicated interplay between reflectance and shape in image measurements.<strong>“</strong><h5 id="2-4-Joint-Acquisition-of-Reflectance-and-Shape"><a href="#2-4-Joint-Acquisition-of-Reflectance-and-Shape" class="headerlink" title="2.4 Joint Acquisition of Reflectance and Shape"></a>2.4 Joint Acquisition of Reflectance and Shape</h5></li><li>Borom Tunwattanapong, Graham Fyffe, Paul Graham, Jay Busch, Xueming Yu, Abhijeet<br>Ghosh, and Paul Debevec. 2013. Acquiring Reflectance and Shape from Continuous<br>Spherical Harmonic Illumination. ACM Trans. Graph. 32, 4, Article 109 (July 2013),<br>12 pages.<br>rotating LED arc去向样品上投射连续的SH pattern.它是基于distant lighting assumption的.per-pixel reflectance maps给每个view计算,然后将其作为shape reconstruction的输入.这个方法不能用于该方法的原因是:为了减少反射率重建的近场效应,需要每个像素精确的3D位置.</li><li>Zhenglong Zhou, Zhe Wu, and Ping Tan. 2013. Multi-view photometric stereo with<br>spatially varying isotropic materials. In CVPR.<br>这篇文章是基于 isotropic reflectance assumption的,不能用于aisotropic的情形.利用多个view ,环状LED灯每次只开一个.先进行isotropic reflectance computation,然后去estimate the geometry.稀疏的灯可以防止per-pixel reflectance estimation</li><li>Michael Holroyd, Jason Lawrence, and Todd Zickler. 2010. A Coaxial Optical Scanner<br>for Synchronous Acquisition of 3D Geometry and Surface Reflectance. ACM Trans.<br>Graph. 29, 4, Article 99 (July 2010), 12 pages.<br>通过投影仪-相机对以及phase-shift pattern,使用建立了一个gantry,来进行几何重建.但是需要对恢复的反射率比较强的prior,,这是由sparse sampling in the angular domain导致的假设.</li><li>Rui Xia, Yue Dong, Pieter Peers, and Xin Tong. 2016. Recovering Shape and Spatiallyvarying Surface Reflectance Under Unknown Illumination. ACM Trans. Graph. 35,<br>6, Article 187 (Nov. 2016), 12 pages<br>利用未知光照的不连续性,从旋转物体的video sequence来恢复形状和各向同性的反射率.<br>而<br>Giljoo Nam, Joo Ho Lee, Diego Gutierrez, and Min H Kim. 2018. Practical SVBRDF<br>acquisition of 3D objects with unstructured flash photography. In SIGGRAPH Asia<br>Technical Papers. 267</li></ul><p><strong>“</strong> take hundreds of flash photographs from multiple views, to compute a 3D geometry and isotropic reflectance expressed as a linear combination of basis materials, via an involved alternating optimization. <strong>“</strong></p><h5 id="2-5-Deep-Learning-Assisted-Modeling"><a href="#2-5-Deep-Learning-Assisted-Modeling" class="headerlink" title="2.5 Deep-Learning-Assisted Modeling"></a>2.5 Deep-Learning-Assisted Modeling</h5><ul><li>对于将Deep neural  network使用到反射率重建中,可以参考下面的文章<br>(1)Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, and Adrien<br>Bousseau. 2018. Single-image SVBRDF Capture with a Rendering-aware Deep<br>Network. ACM Trans. Graph. 37, 4, Article 128 (July 2018), 15 pages.<br>(2)Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. 2017. Modeling Surface Appearance<br>from a Single Photograph Using Self-augmented Convolutional Neural Networks.<br>ACM Trans. Graph. 36, 4, Article 45 (July 2017), 11 pages.</li><li>对于将Deep neural  network使用到形状重建中,可以参考下面的文章<br>(1)Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy,<br>Abraham Bachrach, and Adam Bry. 2017. End-To-End Learning of Geometry and<br>Context for Deep Stereo Regression. In ICCV.<br>(2)Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. 2018. MVSNet: Depth<br>Inference for Unstructured Multi-view Stereo. In ECCV.</li><li>最近</li></ul><p>Shihao Wu, Hui Huang, Tiziano Portenier, Matan Sela, Daniel Cohen-Or, Ron Kimmel, and Matthias Zwicker. 2018. Specular-to-Diffuse Translation for Multi-View<br>Reconstruction. In ECCV.<br>….</p><h4 id="3-ACQUISITION-SETUP"><a href="#3-ACQUISITION-SETUP" class="headerlink" title="3. ACQUISITION SETUP"></a>3. ACQUISITION SETUP</h4><p>见原文</p><h4 id="4-Preliminareis"><a href="#4-Preliminareis" class="headerlink" title="4.Preliminareis"></a>4.Preliminareis</h4><p>在不失一般性的前提下,这篇文章对于上面的acquisition<br>setup做了independently controlled, near-field or distant light sources的假设.并且假设物品是不透明的,可以使用3D mesh进行建模,并且表面形状进行各向异性的SVBRDF建模.偏振过滤器不被使用.同时每个点反射率是单独构建的,没有空间连续性的假设</p><p>关于下面几个公式的解释见上一篇文献阅读文章<br>$$<br>\begin{aligned}<br>B(I, \mathbf{p})=&amp; \int \frac{1}{\left|\mathbf{x}<em>{1}-\mathrm{x}</em>{\mathrm{p}}\right|^{2}} I(l) \Psi\left(\mathrm{x}<em>{1},-\omega</em>{\mathrm{i}}\right) f_{r}\left(\omega_{\mathrm{i}}^{\prime} ; \omega_{\mathbf{o}}^{\prime}, \mathbf{p}\right) \<br>&amp;\left(\omega_{\mathbf{i}} \cdot \mathbf{n}<em>{\mathbf{p}}\right)\left(-\omega</em>{\mathbf{i}} \cdot \mathbf{n}<em>{1}\right) d \mathbf{x}</em>{1}<br>\end{aligned}\<br>L_{0}\left(p, \omega_{0}\right)=L_{e}\left(p, \omega_{0}\right)+\int_{\xi^{2}} f_{r}\left(p, w_{i} \rightarrow w_{0}\right) L_{i}\left(p, \omega_{i}\right) \cos \theta d \omega_{i}\<br>B(I, \mathbf{p})=\sum_{l} I(l) m(l ; \mathbf{p})\<br>m(j ; \mathbf{p})=B({I(l=j)=1, I(l \neq j)=0}, \mathbf{p})<br>$$<br>更进一步的将,lumitexel m可以表示成diffuse lumitexel $m_d$和specular lumitexel $m_s$的和.$m(l)=m_d(l)+m_s(l)$<br>(个人理解,这里应该是将渲染方程中的$f_r$给拆了)</p><h4 id="5-Overview"><a href="#5-Overview" class="headerlink" title="5.Overview"></a>5.Overview</h4><p>本文通过小样本的,同样的lighting pattern下的multi-view photographs,利用mixed-domain neural network取捕捉形状以及反射率.对于每个view中的有效的pixel location,通过投射不同的光照模式,network 将对应点p上面的lumitexel 给encode成少量的measure values.然后将measurements给decode 成diffuse/specular lumitexels,法向向量以及估计的位置.根据不同的view下的这些信息,可以计算出在多角度下的详细的3D mesh.当shape确定了以后,在每一个表面的点上可以将一个4D BRDF以及局部坐标系与lumitexel匹配,然后产生代表了最后的6D SVBRDF的texture map<br><img src="https://img-blog.csdnimg.cn/b025a49249d34ccdb3f2f8adb50782e2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h5 id="5-1-Design-Considerations"><a href="#5-1-Design-Considerations" class="headerlink" title="5.1 Design Considerations"></a>5.1 Design Considerations</h5><ul><li>不会直接使用end-toend(<strong>?</strong>)为最终的几何学习估计的位置p.原因是因为虽然lumitexel中包含了与位置相关的信息(即$\frac{-\omega_i\cdot n_l}{|||x_1-x_p||^2}$),但是这个信息对p的变化太敏感了,很难得到高精度的3D位置信息,但是这个p又需要足够的准确用来在decode diffuse lumitexel for estimating $\rho_d$时eliminate near-field effect.</li><li>不像之前的文章,是利用得到的reflectance来得到normal,而是直接得到这个信息.这是因为在近场条件下,反射率和几何是highly coupled(<strong>“</strong> the reflectance reconstruction requires a position at the current pixel, while the shape reconstruction takes normals as input <strong>“</strong>).本文中利用深度网络来拆解这个mutual dependency.</li><li>与上一篇阅读的文章类似,选择学习lumitexs,而不是直接得到BRDF参数(<strong>“</strong> due to the simple spatially invariant, linear relationship among the lighting pattern, the lumitexel and the measurements <strong>“</strong>).</li><li>现在的主要工作是将multi-view stereo的信息聚合起来建立多个view下的reliable的对应,这是几何重建下重要的异步.但是这个netwok将每一个点的lumitexel当做输入,原因是这使得network简单,并且阻值了**”** the possible combinatorial explosion in synthesizing training data of varied reflectance and shape <strong>“</strong>.并且利用state-of-the-art existing work for spatial aggregation 而不是加重网络的负担.</li></ul><h4 id="6-NETWORK"><a href="#6-NETWORK" class="headerlink" title="6.NETWORK"></a>6.NETWORK</h4>]]></content>
    
    
    
    <tags>
      
      <tag>3D-reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-10-05)Games 102 Geometry-Review</title>
    <link href="/2023/01/10/2021-10-05-Games-102-Geometry-Review/"/>
    <url>/2023/01/10/2021-10-05-Games-102-Geometry-Review/</url>
    
    <content type="html"><![CDATA[<p>由于课件内容较为清楚,此处仅总结重要内容以及其他内容的索引</p><h1 id="1-通论"><a href="#1-通论" class="headerlink" title="1.通论"></a>1.通论</h1><p>对应于课件1</p><ul><li>计算机图形学:表现与表达三维数字对象<br><img src="https://img-blog.csdnimg.cn/c5f68e207edf4403b2b789a668f2edbc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>制作三维数据:几何数据,UV展开,纹理,材质,灯光,动画,…</li><li>几何内容的生成仍然是计算机图形学应用的瓶颈问题之一！<h1 id="2-函数拟合"><a href="#2-函数拟合" class="headerlink" title="2. 函数拟合"></a>2. 函数拟合</h1>对应于课件1,2</li><li>函数空间<br><img src="https://img-blog.csdnimg.cn/25835c304cc8425281324145104239ee.png" alt="在这里插入图片描述"><br>空间的完备性：这个函数空间是否可以表示（逼近）任意函数？</li><li>万能逼近定理(Weierstrass)<br>定理1：闭区间上的连续函数可用多项式级<br>数一致逼近<br>定理2：闭区间上周期为2π的连续函数可用<br>三角函数级数一致逼近</li><li>如何求满足要求的函数?<br><img src="https://img-blog.csdnimg.cn/331e4d38b36a45b8916f5bb8996c13a4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>方法:<br><img src="https://img-blog.csdnimg.cn/f568a8373ac6421ebe64e125b82faa99.png" alt="在这里插入图片描述"><h2 id="2-1-数据拟合"><a href="#2-1-数据拟合" class="headerlink" title="2.1 数据拟合"></a>2.1 数据拟合</h2><h3 id="2-1-1-到哪找"><a href="#2-1-1-到哪找" class="headerlink" title="2.1.1 到哪找?"></a>2.1.1 到哪找?</h3></li><li>选择一个函数空间<br>RBF:径向基函数 <a href="https://www.cnblogs.com/hxsyl/p/5231389.html">https://www.cnblogs.com/hxsyl/p/5231389.html</a><br>成为核函数的基础:Mercer定理<br><a href="https://baike.baidu.com/item/Mercer%E5%AE%9A%E7%90%86/19446322?fr=aladdin">https://baike.baidu.com/item/Mercer%E5%AE%9A%E7%90%86/19446322?fr=aladdin</a></li><li>函数表达为基函数的线性组合<h3 id="2-1-2-找哪个与怎么找"><a href="#2-1-2-找哪个与怎么找" class="headerlink" title="2.1.2 找哪个与怎么找?"></a>2.1.2 找哪个与怎么找?</h3><h4 id="2-1-2-1-目标1"><a href="#2-1-2-1-目标1" class="headerlink" title="2.1.2.1 目标1"></a>2.1.2.1 目标1</h4></li><li>函数要经过每一个数据点 插值(零误差)</li><li>联立,求解线性方程组(用插值多项式可以直接得到结果),但是它们的本质都和线性方程组相关</li><li>病态问题:系数矩阵条件数高时,求解不稳定.</li><li>插值函数的自由度=未知量个数-已知量个数<h4 id="2-1-2-2-目标2"><a href="#2-1-2-2-目标2" class="headerlink" title="2.1.2.2 目标2"></a>2.1.2.2 目标2</h4></li><li>函数尽量靠近数据点(逼近)<br><img src="https://img-blog.csdnimg.cn/515e809e287e46faac5680b9f6a71cb8.png" alt="在这里插入图片描述"></li><li>对个系数求导,得正规方程<h4 id="2-1-2-3-避免过拟合的方法"><a href="#2-1-2-3-避免过拟合的方法" class="headerlink" title="2.1.2.3 避免过拟合的方法"></a>2.1.2.3 避免过拟合的方法</h4></li><li>基函数选择</li><li>岭回归(Ridge regression)–正则约束,还有方差正则项/系数正则项<br><img src="https://img-blog.csdnimg.cn/4212d731fc264f8b97345ecacbe1d984.png" alt="在这里插入图片描述"></li><li>稀疏正则化(冗余基函数,通过优化来选择合适的基函数)—把多余的基函数给筛掉<br><img src="https://img-blog.csdnimg.cn/14c918d381e54a8596cc2446f6bfae6b.png" alt="在这里插入图片描述"> - 稀疏正则化从另一个角度理解:压缩感知(这里实际上是一个重建的问题)<br><img src="https://img-blog.csdnimg.cn/ea284536290349cbbe4c76f1ca88220c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-1-3-分段与光滑与逼近的比较"><a href="#2-1-3-分段与光滑与逼近的比较" class="headerlink" title="2.1.3 分段与光滑与逼近的比较"></a>2.1.3 分段与光滑与逼近的比较</h3></li><li>分段函数数据误差为0,但是只有C0连续,不光滑</li><li>光滑插值误差为0,但是可能被”差数据”带歪,导致函数性质不好,预测不可靠</li><li>逼近拟合函数虽然误差不为0,但是足够小<h3 id="2-1-4-拟合方法"><a href="#2-1-4-拟合方法" class="headerlink" title="2.1.4 拟合方法"></a>2.1.4 拟合方法</h3><h4 id="2-1-4-1-多项式插值"><a href="#2-1-4-1-多项式插值" class="headerlink" title="2.1.4.1 多项式插值"></a>2.1.4.1 多项式插值</h4></li><li>线性方程组矩阵为Vandermonde矩阵,方程组有唯一解</li><li>拉格朗日和牛顿计算公式</li><li>问题:系数矩阵稠密,依赖于基函数的选择,矩阵可能病态,导致难于求解(病态通过条件数衡量)<br><img src="https://img-blog.csdnimg.cn/e02337add4d546f4a145c95fae649fac.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li></ul><p> <strong>注</strong>:相关性的含义是说高次幂函数之间的差别越来越小,这样在拟合时,数据偏一点,就到另一条基函数上去了.</p><ul><li><p>对于等距分布的数据点,范德蒙矩阵的条件数随着数据点数n呈指数级增长</p></li><li><p>好的基函数一般需要系数交替,互相抵消问题(防止基函数长得太近了)</p></li><li><p>解决方法L使用正交多项式基础</p></li><li><p>总结:多项式插值不稳定;Runge现象.</p><h4 id="2-1-4-2-多项式逼近"><a href="#2-1-4-2-多项式逼近" class="headerlink" title="2.1.4.2 多项式逼近"></a>2.1.4.2 多项式逼近</h4></li><li><p>为什么逼近?:数据点含噪声,更紧凑的表达,计算简单/更稳定</p></li><li><p>最小二乘逼近:<br><img src="https://img-blog.csdnimg.cn/6478de48d3bb4238b882368183e8dfae.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/f520f20d2265423b8b8855856aca7e25.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="2-1-4-3-函数空间的选择"><a href="#2-1-4-3-函数空间的选择" class="headerlink" title="2.1.4.3 函数空间的选择"></a>2.1.4.3 函数空间的选择</h4></li><li><p>为什么使用多项式:稠密性与完备性—-表达能力足够</p><h5 id="2-1-4-3-1-Bernstein多项式"><a href="#2-1-4-3-1-Bernstein多项式" class="headerlink" title="2.1.4.3.1 Bernstein多项式"></a>2.1.4.3.1 Bernstein多项式</h5></li><li><p>可以使用Bernstein多项式做逼近<br><img src="https://img-blog.csdnimg.cn/f832315a3d614d0ebe3f4cb32e04c1ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>Bernstein多项式性质良好:凸包性,细分性,递归线性方程求解…</p><h5 id="2-1-4-3-2-RBF函数插值-逼近"><a href="#2-1-4-3-2-RBF函数插值-逼近" class="headerlink" title="2.1.4.3.2 RBF函数插值/逼近"></a>2.1.4.3.2 RBF函数插值/逼近</h5></li><li><p>RBF函数<br><img src="https://img-blog.csdnimg.cn/ec5cd4e655e447ed8ee2cb9f9a7ed7c6.png" alt="在这里插入图片描述"></p></li><li><p>在一维空间中即为Gauss函数<br><img src="https://img-blog.csdnimg.cn/5e8c8a57ceeb4924b8994f05b20c22e6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>现在的问题是RBF函数需要选择,太瘦了重叠部分太小,拟合程度不高</p></li><li><p>解决方法:将均值方差一起带进来优化(只要$a_i,b_i$足够多,也能够稠密)<br><img src="https://img-blog.csdnimg.cn/73eb66c8a5054b2cb45053d4911942fe.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>换个方式看:神经网络</p><h5 id="2-1-4-3-2-神经网络优化"><a href="#2-1-4-3-2-神经网络优化" class="headerlink" title="2.1.4.3.2 神经网络优化"></a>2.1.4.3.2 神经网络优化</h5></li><li><p>上面Guass函数拟合换个方式看:<br><img src="https://img-blog.csdnimg.cn/17dec9abcf0142f88b25637d809fffec.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>神经网络的问题一般在于神经网络的结构与选择</p></li><li><p>Gauss函数的特性:拟局部性(不是凸的,只能得到局部值,比较适合神经网络来做).神经网络万能逼近定理<br><img src="https://img-blog.csdnimg.cn/b81c4a50a3af423c80d3429661d34231.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>多层神经网络:多重复合的函数(一般来说,深的比宽的要好一些)<br><img src="https://img-blog.csdnimg.cn/d18a6334bc0d4b3eb04733ccb737b6c5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>用神经网络来拟合函数<br><img src="https://img-blog.csdnimg.cn/df0722118ff04ccd9e852c735ac0f8a6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li><li><p>Why it works?<br><img src="https://img-blog.csdnimg.cn/c751a310ad834386b5949b23c7070226.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p></li></ul><h1 id="2-曲线拟合"><a href="#2-曲线拟合" class="headerlink" title="2. 曲线拟合"></a>2. 曲线拟合</h1><h2 id="2-1-多元函数与映射"><a href="#2-1-多元函数与映射" class="headerlink" title="2.1 多元函数与映射"></a>2.1 多元函数与映射</h2><p>对应于课件3</p><h3 id="2-1-1-多元函数"><a href="#2-1-1-多元函数" class="headerlink" title="2.1.1 多元函数"></a>2.1.1 多元函数</h3><ul><li>基函数构造:张量积<br><img src="https://img-blog.csdnimg.cn/5b42bfcc237d426891e8f9d981b13c0c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>优点:定义简单;缺点:随着维数增加,基函数个数急剧增加,求解代价大</li><li>用神经网络表达<br><img src="https://img-blog.csdnimg.cn/b1449d2e2a4444d59645482fa1b91d02.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-1-2-流形与映射"><a href="#2-1-2-流形与映射" class="headerlink" title="2.1.2 流形与映射"></a>2.1.2 流形与映射</h3></li><li>流形是一个从低维到高维的映射;任何一个点,无穷小区域等价于一个圆盘</li><li>降维映射(低维映射):降维映射一般有信息丢失,丢失的信息大部分不可逆,无法恢复.但是流形结构是可以恢复的.不可恢复优势也是有用的,比如提取信息,用于自编码器:m–&gt;n–&gt;m.<br>这也启示我们,调参的时候,隐藏层的维度不能太低,神经网络的本质:一个维度映射到另外一个维度</li><li>二维到二维的映射或者三维到三维的映射:plane warp/space warp<h2 id="2-2-曲线拟合基础"><a href="#2-2-曲线拟合基础" class="headerlink" title="2.2 曲线拟合基础"></a>2.2 曲线拟合基础</h2>对应于课件3<br><img src="https://img-blog.csdnimg.cn/0204e0251d464fab940e1cf61582dab0.png" alt="在这里插入图片描述"><h3 id="2-2-1-参数化问题"><a href="#2-2-1-参数化问题" class="headerlink" title="2.2.1 参数化问题"></a>2.2.1 参数化问题</h3>对应于课件3<br>观察上面误差度量的公式中,怎么知道数据点对应的参数?:这是个降维问题.求出参数后,然后极小化误差度量</li><li>参数化方法:<br><img src="https://img-blog.csdnimg.cn/ed8e9e4076614befbea2c14afefd80ed.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/358c9b8629584bebb3e3d892c1010638.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>点的参数化对曲线拟合的影响很大</li></ul><h3 id="2-2-2-曲面参数化"><a href="#2-2-2-曲面参数化" class="headerlink" title="2.2.2 曲面参数化"></a>2.2.2 曲面参数化</h3><p>对应于课件3<br>三维的点找二维的参数,降维问题<br>需要有好的几何性质,保局部的面积/长度/角度等几何性质,是好的.</p><ul><li>应用:地图绘制,纹理映射</li></ul><h2 id="2-3-三次样条函数"><a href="#2-3-三次样条函数" class="headerlink" title="2.3 三次样条函数"></a>2.3 三次样条函数</h2><p>对应于课件4<br><img src="https://img-blog.csdnimg.cn/1826cbc445414a0783af8a8ac8e06c13.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2d3067f8ec5647c2876fb0ae063d5d96.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-3-1-力学解释"><a href="#2-3-1-力学解释" class="headerlink" title="2.3.1 力学解释"></a>2.3.1 力学解释</h3><p>见课件(贝努力-欧拉方程)<br>可以得到两压铁之间的函数为三次函数,即样条曲线为分段三次函数</p><h3 id="2-3-2-数学性质"><a href="#2-3-2-数学性质" class="headerlink" title="2.3.2 数学性质"></a>2.3.2 数学性质</h3><h4 id="2-3-2-1-问题描述"><a href="#2-3-2-1-问题描述" class="headerlink" title="2.3.2.1 问题描述"></a>2.3.2.1 问题描述</h4><p><img src="https://img-blog.csdnimg.cn/059d227265eb455a84144d2f495e68fa.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/4a1c275b9e4d4f85a6873e626cb42a35.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="2-3-2-2-解决方法"><a href="#2-3-2-2-解决方法" class="headerlink" title="2.3.2.2 解决方法"></a>2.3.2.2 解决方法</h4><p><img src="https://img-blog.csdnimg.cn/3925fdedfb4340a6904acabd22a7d813.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/6086ceaf5ea14c8695e0fe9cc899c401.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/fb0066fd4cef4fcfb2ec85a38700d0c1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/625131b043ee4f5981c7de7b13a27512.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><ul><li>在给定一次导数或者二次导数的时候计算三次插值公式,可以使用下面的简化技巧<br><img src="https://img-blog.csdnimg.cn/efcc72521ede49ccac39386264c7949d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/ff1007cdf5be4f7e8c721291061e1d16.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-3-3-三次基样条"><a href="#2-3-3-三次基样条" class="headerlink" title="2.3.3 三次基样条"></a>2.3.3 三次基样条</h3></li></ul><p><strong>B样条教程</strong>:<br><a href="https://blog.csdn.net/tuqu/article/details/5366701">https://blog.csdn.net/tuqu/article/details/5366701</a><br><img src="https://img-blog.csdnimg.cn/c425011d2b474d6188cd5d5067c90a5d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-3-4-三次样条的缺点"><a href="#2-3-4-三次样条的缺点" class="headerlink" title="2.3.4 三次样条的缺点"></a>2.3.4 三次样条的缺点</h3><p><img src="https://img-blog.csdnimg.cn/752015a8f4b64f6b8714cb1619670f74.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="2-3-5-三次参数样条曲线"><a href="#2-3-5-三次参数样条曲线" class="headerlink" title="2.3.5 三次参数样条曲线"></a>2.3.5 三次参数样条曲线</h3><p><img src="https://img-blog.csdnimg.cn/32e9db81f71b42faba98cfb2ecf4c449.png" alt="在这里插入图片描述"></p><h2 id="2-4-几何连续性"><a href="#2-4-几何连续性" class="headerlink" title="2.4 几何连续性"></a>2.4 几何连续性</h2><p>对应于课件4</p><ul><li>参数连续性过于严格，在几何设计中不太直观.连续性依赖于参数的选择，同一条曲线，参数不同，连续阶也不同.只需要在某一段引入一个参数变化,就会导致不连续的产生.</li><li>几何连续性:<br><img src="https://img-blog.csdnimg.cn/3bcfd95be1b74960b8c6a0b5f552af89.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>性质:<br><img src="https://img-blog.csdnimg.cn/e48716bcd93c4fbc96dae346db24128d.png" alt="在这里插入图片描述"></li><li>具体形式: <img src="https://img-blog.csdnimg.cn/f404b7e1c19641e9a943bbf2dcba6119.png" alt="在这里插入图片描述"><br>(G1实际上是将一个C1中的向量相等变成了方向相等)</li></ul><h2 id="2-4-Bezier曲线"><a href="#2-4-Bezier曲线" class="headerlink" title="2.4 Bezier曲线"></a>2.4 Bezier曲线</h2><p>对应于课件5</p><h3 id="2-4-1-基本问题"><a href="#2-4-1-基本问题" class="headerlink" title="2.4.1 基本问题"></a>2.4.1 基本问题</h3><ul><li>基本想法:<br><img src="https://img-blog.csdnimg.cn/47bf9d4a19dc4867b400fbdb15b65f88.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/4da46688330648baba835cf24dd7f57c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>为什么需要直观?这里涉及到建模的两种形式:<br><img src="https://img-blog.csdnimg.cn/39343d114b0e4673b426d3794cf9ab32.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-2-Bernstein基函数"><a href="#2-4-2-Bernstein基函数" class="headerlink" title="2.4.2 Bernstein基函数"></a>2.4.2 Bernstein基函数</h3></li><li>候选方法:使用Bernstein基函数来表达.系数顶点与曲线关联性强,具有很好的几何意义.对于交互式曲线设计更加直观<br><img src="https://img-blog.csdnimg.cn/4ef459dd1f9b40bb8a7bb52a65658984.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><ul><li>几何性质:<br><img src="https://img-blog.csdnimg.cn/67954c1e6f4d48f8b946bef844edc531.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><h3 id="2-4-3-Bezier曲线"><a href="#2-4-3-Bezier曲线" class="headerlink" title="2.4.3 Bezier曲线"></a>2.4.3 Bezier曲线</h3></li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>geoemtry</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-10-01)slam 14 courses-review</title>
    <link href="/2023/01/10/2021-10-01-slam-14-courses-review/"/>
    <url>/2023/01/10/2021-10-01-slam-14-courses-review/</url>
    
    <content type="html"><![CDATA[<p>由于书上讲的比较清楚，此处仅记录重点知识，梳理相关代码。</p><h1 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1.环境配置"></a>1.环境配置</h1><h2 id="1-1-cmake"><a href="#1-1-cmake" class="headerlink" title="1.1 cmake"></a>1.1 cmake</h2><h3 id="1-1-1-基本使用"><a href="#1-1-1-基本使用" class="headerlink" title="1.1.1 基本使用"></a>1.1.1 基本使用</h3><ul><li>最基础的cmake 相关指令:project() 与 add_executable<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c"># 声明要求的 cmake 最低版本<br> cmake_minimum_required( VERSION <span class="hljs-number">2.8</span> )<br> # 声明一个 cmake 工程<br> project( HelloSLAM )<br> # 添加一个可执行程序<br> # 语法：add_executable( 程序名 源代码文件 ）<br> add_executable( helloSLAM helloSLAM.cpp )<br></code></pre></td></tr></table></figure></li><li>相关使用<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c">mkdir build<br>cd build<br>cmake ..<br>make<br></code></pre></td></tr></table></figure><h3 id="1-1-2-生成库"><a href="#1-1-2-生成库" class="headerlink" title="1.1.2 生成库"></a>1.1.2 生成库</h3>将add_executable替换成add_library(.a后缀)<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">add_library( hello libHelloSLAM.cpp )<br></code></pre></td></tr></table></figure>如果生成共享库(.so后缀)<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">add_library( hello_shared SHARED libHelloSLAM.cpp )<br></code></pre></td></tr></table></figure>除了库文件,还需要提供一个头文件.头文件和库文件需要同时提供给用户<h3 id="1-1-3-使用库"><a href="#1-1-3-使用库" class="headerlink" title="1.1.3 使用库"></a>1.1.3 使用库</h3></li><li>如果库与源代码在同一个位置下<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">add_executable( useHello useHello.cpp )<br>target_link_libraries( useHello hello_shared )<br></code></pre></td></tr></table></figure></li><li>如果库与源代码不在同一个位置下</li></ul><h2 id="1-2-IDE-Kdevelop"><a href="#1-2-IDE-Kdevelop" class="headerlink" title="1.2 IDE Kdevelop"></a>1.2 IDE Kdevelop</h2><p>见34</p><h1 id="2-知识部分"><a href="#2-知识部分" class="headerlink" title="2.知识部分"></a>2.知识部分</h1><h2 id="2-1-基础数学知识"><a href="#2-1-基础数学知识" class="headerlink" title="2.1 基础数学知识"></a>2.1 基础数学知识</h2><h3 id="2-1-1-外积的表示"><a href="#2-1-1-外积的表示" class="headerlink" title="2.1.1 外积的表示"></a>2.1.1 外积的表示</h3><p>p42</p><ul><li>对于向量引入运算^,该运算表示<br>$$<br>\left[\begin{array}{ccc}<br>0 &amp; -a_{3} &amp; a_{2} \<br>a_{3} &amp; 0 &amp; -a_{1} \</li><li>a_{2} &amp; a_{1} &amp; 0<br>\end{array}\right]<br>$$<br>从而 $a\times b$=$a$^$b$</li><li>坐标系之间的欧式变换<br>$$<br>\left[e_{1}, e_{2}, e_{3}\right]\left[\begin{array}{l}<br>a_{1} \<br>a_{2} \<br>a_{3}<br>\end{array}\right]=\left[e_{1}^{\prime}, e_{2}^{\prime}, e_{3}^{\prime}\right]\left[\begin{array}{c}<br>a_{1}^{\prime} \<br>a_{2}^{\prime} \<br>a_{3}^{\prime}<br>\end{array}\right]\Rightarrow\<br>\left[\begin{array}{l}<br>a_{1} \<br>a_{2} \<br>a_{3}<br>\end{array}\right]=\left[\begin{array}{lll}<br>e_{1}^{T} e_{1}^{\prime} &amp; e_{1}^{T} e_{2}^{\prime} &amp; e_{1}^{T} e_{3}^{\prime} \<br>e_{2}^{T} e_{1}^{\prime} &amp; e_{2}^{T} e_{2}^{\prime} &amp; e_{2}^{T} e_{3}^{\prime} \<br>e_{3}^{T} e_{1}^{\prime} &amp; e_{3}^{T} e_{2}^{\prime} &amp; e_{3}^{T} e_{3}^{\prime}<br>\end{array}\right]\left[\begin{array}{l}<br>a_{1}^{\prime} \<br>a_{2}^{\prime} \<br>a_{3}^{\prime}<br>\end{array}\right] \triangleq \boldsymbol{R} \boldsymbol{a}^{\prime}<br>$$<br>中间的矩阵记为R,其中$R^{-1}=R^T$</li></ul><h3 id="2-1-2-齐次坐标与特殊欧式群"><a href="#2-1-2-齐次坐标与特殊欧式群" class="headerlink" title="2.1.2 齐次坐标与特殊欧式群"></a>2.1.2 齐次坐标与特殊欧式群</h3><p>p46</p><ul><li>齐次坐标下的变化可以表示成特殊欧式群中的元素<br>$$<br>S E(3)=\left{T=\left[\begin{array}{cc}<br>R &amp; t \<br>0^{T} &amp; 1<br>\end{array}\right] \in \mathbb{R}^{4 \times 4} \mid R \in S O(3), t \in \mathbb{R}^{3}\right}<br>$$<h3 id="2-1-3-旋转向量与欧拉角"><a href="#2-1-3-旋转向量与欧拉角" class="headerlink" title="2.1.3 旋转向量与欧拉角"></a>2.1.3 旋转向量与欧拉角</h3>p50<h4 id="2-1-3-1-旋转向量"><a href="#2-1-3-1-旋转向量" class="headerlink" title="2.1.3.1 旋转向量"></a>2.1.3.1 旋转向量</h4></li><li>起因:SO(3)的旋转矩阵有九个量,但是一次旋转只有三个自由度;同时旋转矩阵自身带有约束,不利于求解</li><li>旋转向量:方向与旋转轴一致,而长度等于旋转角.事实上,旋转向量即使李代数.</li></ul><h4 id="2-1-3-2-罗德里格斯公式"><a href="#2-1-3-2-罗德里格斯公式" class="headerlink" title="2.1.3.2 罗德里格斯公式"></a>2.1.3.2 罗德里格斯公式</h4><p>-旋转向量到旋转矩阵的转换,公式为<br>$$<br>\boldsymbol{R}=\cos \theta \boldsymbol{I}+(1-\cos \theta) \boldsymbol{n} \boldsymbol{n}^{T}+\sin \theta \boldsymbol{n}^{\wedge}<br>$$</p><ul><li>旋转矩阵到旋转向量的转换,公式为<br>$$<br>\begin{aligned}<br>\operatorname{tr}(\boldsymbol{R}) &amp;=\cos \theta \operatorname{tr}(\boldsymbol{I})+(1-\cos \theta) \operatorname{tr}\left(\boldsymbol{n} \boldsymbol{n}^{T}\right)+\sin \theta \operatorname{tr}\left(\boldsymbol{n}^{\wedge}\right) \<br>&amp;=3 \cos \theta+(1-\cos \theta) \<br>&amp;=1+2 \cos \theta<br>\end{aligned}<br>$$<br>因此:<br>$$<br>\theta=\arccos \left(\frac{\operatorname{tr}(\boldsymbol{R})-1}{2}\right)<br>$$<br>关于转轴 $n$, 由于旋转轴上的向量在旋转后不发生改变, 说明<br>$$<br>R n=n<br>$$<br>因此，转轴 $n$ 是矩阵 $R$ 特征值 1 对应的特征向量。求解此方程, 再归一化, 就得到旋转轴</li></ul><h4 id="2-1-3-3-欧拉角"><a href="#2-1-3-3-欧拉角" class="headerlink" title="2.1.3.3 欧拉角"></a>2.1.3.3 欧拉角</h4><ul><li>将旋转分解为三次绕不同轴的旋转,顺序为ZYX轴的旋转,三个角度为“偏航-俯仰-滚转”（yaw-pitch-roll）<br>用$[r,p,y]^T$表示</li><li>万向锁问题:在俯仰角为±90◦ 时，第一次旋转与第三次旋转将使用同一个轴，使得系统丢失了一个自由度.<br>(理论上来首,三个实数表示三维旋转都有该问题,所以欧拉角<strong>不适合插值和迭代,只能用于人机交互</strong>)<br><img src="https://img-blog.csdnimg.cn/aa1960d79a324003b5d55957f3e9d6ad.png" alt="在这里插入图片描述"><h3 id="2-1-4-四元数"><a href="#2-1-4-四元数" class="headerlink" title="2.1.4 四元数"></a>2.1.4 四元数</h3><h4 id="2-1-4-1-四元数定义"><a href="#2-1-4-1-四元数定义" class="headerlink" title="2.1.4.1 四元数定义"></a>2.1.4.1 四元数定义</h4></li><li>找不到不带奇异性的三维向量的描述方式(三维旋转是一个三维流形,想要无奇异性地表达它,三个量是不够的)</li><li>四元数是紧凑的,也没有奇异性<br>$$<br>q=q_{0}+q_{1} i+q_{2} j+q_{3} k<br>$$<br>其中 $i, j, k$ 为四元数的三个虚部<br>$$<br>\left{\begin{array}{l}<br>i^{2}=j^{2}=k^{2}=-1 \<br>i j=k, j i=-k \<br>j k=i, k j=-i \<br>k i=j, i k=-j<br>\end{array}\right.<br>$$<br>也可以表示成<br>$$<br>\boldsymbol{q}=[s, \boldsymbol{v}], \quad s=q_{0} \in \mathbb{R}, \boldsymbol{v}=\left[q_{1}, q_{2}, q_{3}\right]^{T} \in \mathbb{R}^{3}<br>$$<br>这里, $s$ 称为四元数的实部, 而 $v$ 称为它的虚部。</li><li>旋转向量转换为<h4 id="2-1-4-2-四元数的运算"><a href="#2-1-4-2-四元数的运算" class="headerlink" title="2.1.4.2 四元数的运算"></a>2.1.4.2 四元数的运算</h4></li><li>乘法</li><li>共轭</li><li>模长</li><li>逆</li><li>数乘与点乘<h4 id="2-1-4-3-四元数与旋转"><a href="#2-1-4-3-四元数与旋转" class="headerlink" title="2.1.4.3 四元数与旋转"></a>2.1.4.3 四元数与旋转</h4></li><li>单元四元数表示空间中的一个旋转,乘以i表示绕着i轴旋转了180度(虽然i^2=-1)<h5 id="2-1-4-3-1-四元数表示旋转"><a href="#2-1-4-3-1-四元数表示旋转" class="headerlink" title="2.1.4.3.1 四元数表示旋转"></a>2.1.4.3.1 四元数表示旋转</h5>首先, 把三维空间点用一个虚四元数来描述:<br>$$<br>\boldsymbol{p}=[0, x, y, z]=[0, \boldsymbol{v}]<br>$$<br>这相当于我们把四元数的三个虚部与空间中的三个轴相对应。然后, 参照式 (3.19), 用四 元数 $q$ 表示这个旋转:<br>$$<br>\boldsymbol{q}=\left[\cos \frac{\theta}{2}, \boldsymbol{n} \sin \frac{\theta}{2}\right]<br>$$<br>那么, 旋转后的点 $p^{\prime}$ 即可表示为这样的乘积:<br>$$<br>p^{\prime}=q p q^{-1}<br>$$<br>计算结果的实部为 0 , 故为纯虚四元数。其虚部的三个分量表 示旋转后 $3 \mathrm{D}$ 点的坐标。<h5 id="2-1-4-3-2-四元数与旋转向量的转换"><a href="#2-1-4-3-2-四元数与旋转向量的转换" class="headerlink" title="2.1.4.3.2 四元数与旋转向量的转换"></a>2.1.4.3.2 四元数与旋转向量的转换</h5></li><li>从旋转向量到四元数<br>$$<br>\boldsymbol{q}=\left[\cos \frac{\theta}{2}, n_{x} \sin \frac{\theta}{2}, n_{y} \sin \frac{\theta}{2}, n_{z} \sin \frac{\theta}{2}\right]^{T}<br>$$</li><li>从四元数到旋转响铃<br>$$<br>\left{\begin{array}{l}<br>\theta=2 \arccos q_{0} \<br>{\left[n_{x}, n_{y}, n_{z}\right]^{T}=\left[q_{1}, q_{2}, q_{3}\right]^{T} / \sin \frac{\theta}{2}}<br>\end{array}\right.<br>$$<br>(旋转$2\pi$得到的是相反的四元数,虽然旋转到了原处)<h5 id="2-1-4-3-3-四元数与旋转矩阵的转换"><a href="#2-1-4-3-3-四元数与旋转矩阵的转换" class="headerlink" title="2.1.4.3.3 四元数与旋转矩阵的转换"></a>2.1.4.3.3 四元数与旋转矩阵的转换</h5>通过将四元数转换为旋转向量,再转换为矩阵需要计算arccos,太麻烦,通过下面的工作计算.<br>设四元数 $q=q_{0}+q_{1} i+q_{2} j+q_{3} k$, 对应的旋转矩阵 $R$ 为:<br>$$<br>\boldsymbol{R}=\left[\begin{array}{ccc}<br>1-2 q_{2}^{2}-2 q_{3}^{2} &amp; 2 q_{1} q_{2}+2 q_{0} q_{3} &amp; 2 q_{1} q_{3}-2 q_{0} q_{2} \<br>2 q_{1} q_{2}-2 q_{0} q_{3} &amp; 1-2 q_{1}^{2}-2 q_{3}^{2} &amp; 2 q_{2} q_{3}+2 q_{0} q_{1} \<br>2 q_{1} q_{3}+2 q_{0} q_{2} &amp; 2 q_{2} q_{3}-2 q_{0} q_{1} &amp; 1-2 q_{1}^{2}-2 q_{2}^{2}<br>\end{array}\right]<br>$$<br>反之，由旋转矩阵到四元数的转换如下。假设矩阵为 $\boldsymbol{R}=\left{m_{i j}\right}, i, j \in[1,2,3]$, 其对 应的四元数 $q$ 由下式给出:<br>$$<br>q_{0}=\frac{\sqrt{\operatorname{tr}(R)+1}}{2}, q_{1}=\frac{m_{23}-m_{32}}{4 q_{0}}, q_{2}=\frac{m_{31}-m_{13}}{4 q_{0}}, q_{3}=\frac{m_{12}-m_{21}}{4 q_{0}}<br>$$</li><li>值得一提的是，由于 $\boldsymbol{q}$ 和 $-\boldsymbol{q}$ 表示同一个旋转, 事实上一个 $\boldsymbol{R}$ 对应的四元数表示并 不是惟一的。</li><li>实际编程中,当 $q_{0}$ 接近 0 时, 其余三个分量会非常大，导致解不稳定，此时我们 再考虑使用其他的方式进行转换。</li></ul><h5 id="2-1-4-3-4-四元数的运算"><a href="#2-1-4-3-4-四元数的运算" class="headerlink" title="2.1.4.3.4  四元数的运算"></a>2.1.4.3.4  四元数的运算</h5><ul><li>乘法:<br>$$<br>\begin{aligned}<br>\boldsymbol{q}<em>{a} \boldsymbol{q}</em>{b}=&amp; s_{a} s_{b}-x_{a} x_{b}-y_{a} y_{b}-z_{a} z_{b} \<br>&amp;+\left(s_{a} x_{b}+x_{a} s_{b}+y_{a} z_{b}-z_{a} y_{b}\right) i \<br>&amp;+\left(s_{a} y_{b}-x_{a} z_{b}+y_{a} s_{b}+z_{a} x_{b}\right) j \<br>&amp;+\left(s_{a} z_{b}+x_{a} y_{b}-y_{b} x_{a}+z_{a} s_{b}\right) k<br>\end{aligned}\<br>\boldsymbol{q}<em>{a} \boldsymbol{q}</em>{b}=[s_as_b-\boldsymbol{v}<em>{a} ^T\boldsymbol{v}</em>{b},s_a\boldsymbol{v}<em>{b}+s_b\boldsymbol{v}</em>{a}+\boldsymbol{v}<em>{a}\times \boldsymbol{v}</em>{b}]<br>$$</li><li>共轭<br>四元数的共轭是把虚部取成相反数:<br>$$<br>\boldsymbol{q}<em>{a}^{*}=s</em>{a}-x_{a} i-y_{a} j-z_{a} k=\left[s_{a},-\boldsymbol{v}<em>{a}\right]<br>$$<br>四元数共轭与自己本身相乘，会得到一个实四元数, 其实部为模长的平方:<br>$$<br>q^{<em>} \boldsymbol{q}=\boldsymbol{q} \boldsymbol{q}^{</em>}=\left[s</em>{a}^{2}+\boldsymbol{v}^{T} \boldsymbol{v}, \mathbf{0}\right]<br>$$</li><li>逆<br>$$<br>q^{-1}=q^*/||q||^2,(q_aq_b)^{-1}=(q_b)^{-1}(q_a)^{-1}<br>$$</li><li>数乘与点乘 类似于向量的数乘和点乘/<h5 id="2-1-5-相似-仿射-射影变换"><a href="#2-1-5-相似-仿射-射影变换" class="headerlink" title="2.1.5 相似/仿射/射影变换"></a>2.1.5 相似/仿射/射影变换</h5></li><li>相似变换<br>$$<br>T_{S}=\left[\begin{array}{ll}<br>s \boldsymbol{R} &amp; \boldsymbol{t} \<br>\mathbf{0}^{T} &amp; 1<br>\end{array}\right]<br>$$</li><li>仿射变换<br>$$<br>T_{A}=\left[\begin{array}{ll}<br>A &amp; \boldsymbol{t} \<br>\mathbf{0}^{T} &amp; 1<br>\end{array}\right]<br>$$<br>其中只要求A是一个可逆矩阵,而不是正交矩阵</li><li>射影变换<br>$$<br>T_{P}=\left[\begin{array}{ll}<br>A &amp; \boldsymbol{t} \<br>\mathbf{a}^{T} &amp; 1<br>\end{array}\right]<br>$$<br>可以对真个矩阵除以v得到一个右下角为1的矩阵,此时有15个自由度</li><li>从真实世界到相机照片的变换可以看成一个射影变化,如果相机的i焦距为无穷远,那么这个变换则为仿射变换<h1 id="3-代码部分"><a href="#3-代码部分" class="headerlink" title="3.代码部分"></a>3.代码部分</h1></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>slam</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>(2021-09-28)3D-Reconstruction Review(1)</title>
    <link href="/2023/01/10/2021-09-28-3D-Reconstruction-Review-1/"/>
    <url>/2023/01/10/2021-09-28-3D-Reconstruction-Review-1/</url>
    
    <content type="html"><![CDATA[<h1 id="Fundamentally-Reading"><a href="#Fundamentally-Reading" class="headerlink" title="Fundamentally Reading"></a>Fundamentally Reading</h1><p>(仅作为学习过程中的记录,由于是最开始几篇相关论文,读的比较细)</p><h2 id="Efficient-Reflectance-Capture-Using-an-Autoencoder"><a href="#Efficient-Reflectance-Capture-Using-an-Autoencoder" class="headerlink" title="Efficient Reflectance Capture Using an Autoencoder"></a>Efficient Reflectance Capture Using an Autoencoder</h2><h3 id="1-问题介绍"><a href="#1-问题介绍" class="headerlink" title="1.问题介绍"></a>1.问题介绍</h3><p>渲染方程:<a href="https://zhuanlan.zhihu.com/p/52497510?from_voters_page=true">https://zhuanlan.zhihu.com/p/52497510?from_voters_page=true</a></p><p>BRDF概念:<a href="https://www.cnblogs.com/mengdd/archive/2013/08/05/3237991.html">https://www.cnblogs.com/mengdd/archive/2013/08/05/3237991.html</a></p><p>batch normalization layer概念:<a href="https://zhuanlan.zhihu.com/p/74516930">https://zhuanlan.zhihu.com/p/74516930</a></p><p>stacked neural network这里面提到了一点:<a href="https://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html">https://www.cnblogs.com/tornadomeet/archive/2013/05/05/3061457.html</a></p><p>渲染方程中什么叫lobe:<a href="https://www.zhihu.com/question/314492478">https://www.zhihu.com/question/314492478</a></p><h3 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h3><h5 id="2-1-综述"><a href="#2-1-综述" class="headerlink" title="2.1 综述"></a>2.1 综述</h5><p>Tim Weyrich, Jason Lawrence, Hendrik P. A. Lensch, Szymon Rusinkiewicz, and Todd<br>Zickler. 2009. Principles of Appearance Acquisition and Representation. Found.<br>Trends. Comput. Graph. Vis. 4, 2 (2009), 75–191.<br>Michael Weinmann and Reinhard Klein. 2015. Advances in Geometry and Reflectance<br>Acquisition. In SIGGRAPH Asia Courses. Article 1, 71 pages.</p><h5 id="2-2-Direct-Sampling"><a href="#2-2-Direct-Sampling" class="headerlink" title="2.2 Direct Sampling"></a>2.2 Direct Sampling</h5><ul><li>Kristin J. Dana, Bram van Ginneken, Shree K. Nayar, and Jan J. Koenderink. 1999.<br>Reflectance and Texture of Real-world Surfaces. ACM Trans. Graph. 18, 1 (Jan. 1999),<br>1–34.<br>对大量的光照和观察角度进行采样,并穷举这两个因素的任意组合</li></ul><p><strong>评述</strong> :typically time-consuming</p><ul><li>下面的方法在尝试通过对数据进行假设,减少该消耗,减少图片数量.</li><li>Stephen R. Marschner, Stephen H. Westin, Eric P. F. Lafortune, Kenneth E. Torrance,<br>and Donald P. Greenberg. 1999. Image-based BRDF Measurement Including Human<br>Skin. In Proc. EGWR. 131–144.<br>homogeneous的凸图形可以通过一个方向的观察view就能得到反射率(通过利用 normal variations对angular domain进行足够的采样)</li><li>Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter<br>Seidel. 2003. Image-based Reconstruction of Spatial Appearance and Geometric<br>Detail. ACM Trans. Graph. 22, 2 (April 2003), 234–257.<br>假设外观(appearance)是基本材料的线性组合的话,形状已知图形的反射率(尽管处处不同)通过少量的照片可以重建</li><li>Todd Zickler, Sebastian Enrique, Ravi Ramamoorthi, and Peter Belhumeur. 2005. Reflectance Sharing: Image-based Rendering from a Sparse Set of Images. In Proc. EGSR.<br>253–264.通过6D空间中的反射信息,对分散的数据进行插值( scattered-data interpolation)进行重建</li><li>Jiaping Wang, Shuang Zhao, Xin Tong, John Snyder, and Baining Guo. 2008. Modeling<br>Anisotropic Surface Reflectance with Example-based Microfacet Synthesis. ACM<br>Trans. Graph. 27, 3, Article 41 (Aug. 2008), 9 pages.<br>利用反射率空间上的相似性,以及局部坐标系空间的变化,通过单角度的衡量,来完成BRDFS在的微表面分布(microfacet distributions).</li><li>Yue Dong, Jiaping Wang, Xin Tong, John Snyder, Yanxiang Lan, Moshe Ben-Ezra, and<br>Baining Guo. 2010. Manifold Bootstrapping for SVBRDF Capture. ACM Trans.<br>Graph. 29, 4, Article 98 (July 2010), 10 pages.<br>假设反射率在一个低维流形上,提出两阶段识别模型</li><li>Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. 2015. Two-shot SVBRDF Capture for<br>Stationary Materials. ACM Trans. Graph. 34, 4, Article 110 (July 2015), 13 pages.<br>利用两张图片取建模stochastic-texture-like采莲的外观</li><li>上述文章仅作为比较。该文章 In particular, we reconstruct the reflectance at each<br>point independently, despite the low number of measurements.（<strong>？</strong>）<br>同时，通过训练本文中的autoencoder，额外的材料的性质很容易被利用（不用手动推导）</li></ul><h5 id="2-3-Complex-Lighting-Patterns"><a href="#2-3-Complex-Lighting-Patterns" class="headerlink" title="2.3 Complex Lighting Patterns"></a>2.3 Complex Lighting Patterns</h5><ul><li>与本文的方法类似.记录不同光照模式(lighting pettern)下样本的反映来恢复反射性质.</li><li>Abhijeet Ghosh, Tongbo Chen, Pieter Peers, Cyrus A. Wilson, and Paul Debevec. 2009.<br>Estimating Specular Roughness and Anisotropy from Second Order Spherical Gradient Illumination. Computer Graphics Forum 28, 4 (2009), 1161–1170.<br>采用spherical harmonics (SH)光照模式,,然后利用一张手工推导的反向查询表进行恢复.该反向查询表建立了the observed<br>radiance to anisotropic BRDF parameters的映射关系</li><li>Giljoo Nam, Joo Ho Lee, Hongzhi Wu, Diego Gutierrez, and Min H. Kim. 2016. Simultaneous Acquisition of Microscale Reflectance and Normals. ACM Trans. Graph. 35, 6,<br>Article 185 (Nov. 2016), 11 pages.<br>提出一个相似的系统,在较少数量的基本材料的假设下,可以通过自动优化重建micro-scale的反射率</li><li>上述的系统都假设了入射光离得较远(相对于样品的尺寸来说)</li><li>Andrew Gardner, Chris Tchou, Tim Hawkins, and Paul Debevec. 2003. Linear light<br>source reflectometry. ACM Trans. Graph. 22, 3 (2003), 749–758.<strong>and</strong> Peiran Ren, Jiaping Wang, John Snyder, Xin Tong, and Baining Guo. 2011. Pocket<br>reflectometry. ACM Trans. Graph. 30, 4 (2011), 1–10.<br>线性光源,平面的各向同型的材料样品.<strong>and</strong><br>Guojun Chen, Yue Dong, Pieter Peers, Jiawan Zhang, and Xin Tong. 2014. Reflectance<br>Scanning: Estimating Shading Frame and BRDF with Generalized Linear Light<br>Sources. ACM Trans. Graph. 33, 4, Article 117 (July 2014), 11 pages<br>将其扩展到各向异性的发射率,方法是通过调整线性光源(linear light source)的强度,并假设外观子空间的低秩性.</li><li>Miika Aittala, Tim Weyrich, and Jaakko Lehtinen. 2013. Practical SVBRDF Capture in<br>the Frequency Domain. ACM Trans. Graph. 32, 4, Article 110 (July 2013), 12 pages.<br>提出一个单个相机,倾斜的近场LCD板作为可编程的面光源,来得到各向同性的反射率.它基于手动的频域分析.</li><li>本文使用16~32张光照模式,高效可信的得到在空间中变化的各向异性的BRDFs以及局部坐标系.相关工作及其依赖于手动推导,而本文通过机器学习的方法,自动的决定采用什么样的lighting pattern和什么方法去恢复.<h5 id="2-4-Deep-Learning-Assisted-Reflectance-Modeling"><a href="#2-4-Deep-Learning-Assisted-Reflectance-Modeling" class="headerlink" title="2.4 Deep-Learning-Assisted Reflectance Modeling"></a>2.4 Deep-Learning-Assisted Reflectance Modeling</h5></li><li>近年来(指2018年),deep learning的技巧仅能用于基于单图片的反射率重建问题.</li><li>Miika Aittala, Timo Aila, and Jaakko Lehtinen. 2016. Reflectance Modeling by Neural<br>Texture Synthesis. ACM Trans. Graph. 35, 4, Article 65 (July 2016), 13 pages.<br>通过对单张静止的 textured material的flash image分析,为各向同性的SVBRDF和表面法向量建模.困难之处在于精确的点点对应,他们利用CNN,采用纹理的描述子对其进行规避.</li><li>Xiao Li, Yue Dong, Pieter Peers, and Xin Tong. 2017. Modeling Surface Appearance<br>from a Single Photograph Using Self-augmented Convolutional Neural Networks.<br>ACM Trans. Graph. 36, 4, Article 45 (July 2017), 11 pages.<br>present a CNN-based solution for modeling<br>SVBRDF from a single photograph of a planar sample with unknown<br>natural illumination, using a self-augmentation training process</li><li>对于这类方法可以参考下面的论文</li></ul><p> <strong>deep autoencoders:</strong> G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with<br>Neural Networks. Science 313, 5786 (2006), 504–507.<br><strong>deep learning techniques:</strong> Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.<br><a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org</a>.</p><h3 id="3-基础知识"><a href="#3-基础知识" class="headerlink" title="3.基础知识"></a>3.基础知识</h3><h5 id="3-1-渲染方程"><a href="#3-1-渲染方程" class="headerlink" title="3.1 渲染方程"></a>3.1 渲染方程</h5><p>首先来推导一下,为什么渲染方程长这个样子</p><p>$$<br>B(I, \mathbf{p})=\int \frac{1}{|\mathbf{x}<em>{1}-\mathrm{x}</em>{\mathrm{p}}|^{2}} I(l) \Psi(\mathrm{x}<em>{1},-\omega</em>{\mathrm{i}}) f_{r}(\omega_{\mathrm{i}}^{\prime} ; \omega_{\mathbf{o}}^{\prime}, \mathbf{p})<br>(\omega_{\mathbf{i}} \cdot \mathbf{n}<em>{\mathbf{p}})(-\omega</em>{\mathbf{i}} \cdot \mathbf{n}<em>{1}) d \mathbf{x}</em>{1}<br>$$</p><p>其中:<strong>“</strong><br>We model each light as a locally planar source. $\mathrm{x}<em>{\mathrm{p}}/\mathrm{n}</em>{\mathrm{p}}$ are the position / normal of a point $\mathrm{p}$ on the physical sample, and $\mathrm{x}<em>{1} / \mathrm{n}</em>{\mathrm{l}}$ are the position / normal of a point on a light source l. $\omega_{\mathrm{i}} / \omega_{\mathbf{o}}$ are the lighting / view directions in the world space, while $\omega_{\mathrm{i}}^{\prime} / \omega_{\mathbf{o}}{ }^{\prime}$ are their counterparts expressed in the local frame of p. $\omega_{\mathrm{i}}$ can be computed as $\omega_{\mathbf{i}}=\frac{\mathrm{x}<em>{1}-\mathrm{x}</em>{\mathrm{p}}}{\left|\mathrm{x}<em>{1}-\mathrm{x}</em>{\mathrm{p}}\right|} \cdot I(l)$ is the programmable intensity for the light $l$ over its maximum intensity, in the range of $[0,1]$. The array ${I(l)}<em>{l}$ corresponds to a lighting pattern. $\Psi\left(\mathrm{x}</em>{1}, \cdot\right)$ describes the angular distribution of the light intensity when fully on. $f_{r}\left(\cdot ; \omega_{0}^{\prime}, p\right)$ is a $2 D$ BRDF slice, which is a function of the lighting direction only. The above integral is computed over all light sources.<br><strong>“</strong><br>回顾前面的知乎上面提到的渲染方程长这样子的:<br>$$<br>L_{0}\left(p, \omega_{0}\right)=L_{e}\left(p, \omega_{0}\right)+\int_{\xi^{2}} f_{r}\left(p, w_{i} \rightarrow w_{0}\right) L_{i}\left(p, \omega_{i}\right) \cos \theta d \omega_{i}<br>$$<br>$L_{0}\left(p, \omega_{0}\right):$ 最后观察到的辐射度<br>$p$ ：我们想要得到辐射度的这个点<br>$\omega_{0}$ :这个点的方向，法线<br>$L_{e}\left(p, \omega_{0}\right):$ 出射辐射度<br>$\xi^{2}$ :半球的各个方向<br>$f_{r}$ :散射函数<br>$L_{i}$ :入射辐射度<br>$\omega_{i}$ :入射方向<br>$\vartheta$ ：传入方向与法线的夹角</p><ul><li>由于$\omega_i=\frac{x_l-x_p}{||x_l-x_p||}$,对其求导可得$d\omega_i=\frac{x_l}{||x_l-x_p||^2}$,从而对半球上的全方向积分$\int_{\xi^2}dw_i=\int \frac{x_l}{||x_l-x_p||^2}$</li><li>$\vartheta=\omega_i\cdot n_p$</li><li>注意到L的概念:<img src="https://img-blog.csdnimg.cn/a07e74a1e6ff4780ac0b85b5df99037e.png" alt="在这里插入图片描述"><br>所以$I(l)\Phi(x_l,-\omega_i)(\omega_i\cdot n_l)=I(l)\Phi(x_l,-\omega_i)cos\theta=L_i(p,w_i)$<br>(符号有点乱,但要表达的就是这个意思)</li></ul><h5 id="3-2-BRDF的计算"><a href="#3-2-BRDF的计算" class="headerlink" title="3.2 BRDF的计算"></a>3.2 BRDF的计算</h5><p>本文的计算框架没有和任意的BRDF模型绑定(这里应该采用的是一个比较通用的),所以采用下面论文中的模型<br>Bruce Walter, Stephen R. Marschner, Hongsong Li, and Kenneth E. Torrance. 2007.<br>Microfacet Models for Refraction through Rough Surfaces. In Rendering Techniques<br>(Proc. EGWR).<br>$$<br>\begin{aligned}<br>&amp; f_{r}\left(\omega_{\mathrm{i}} ; \omega_{\mathbf{o}}, \mathbf{p}\right) \<br>=&amp; \frac{\rho_{d}}{\pi}+\rho_{s} \frac{D_{\mathrm{GGX}}\left(\omega_{\mathbf{h}} ; \alpha_{x}, \alpha_{y}\right) F\left(\omega_{\mathbf{i}}, \omega_{\mathbf{h}}\right) G_{\mathrm{GG}}\left(\omega_{\mathbf{i}}, \omega_{\mathbf{o}} ; \alpha_{\mathbf{x}}, \alpha_{\mathbf{y}}\right)}{4\left(\omega_{\mathbf{i}} \cdot \mathbf{n}\right)\left(\omega_{\mathbf{o}} \cdot \mathbf{n}\right)}<br>\end{aligned}<br>$$<br><strong>“</strong><br>where $\rho_{d} / \rho_{s}$ are the diffuse / specular albedo, $\alpha_{x} / \alpha_{y}$ are the roughnesses parameters, and $\omega_{h}$ is the half vector. $D_{\mathrm{GGX}}$ is the microfacet distribution function, $F$ is the Fresnel term and $G_{\text {GGX }}$ accounts for shadowing / masking effects, all of which are detailed in the supplemental material for brevity.<br><strong>“</strong><br>注:反射率reflectance,反照率 albedo 他们的区别是<a href="https://blog.csdn.net/qq_35045096/article/details/91949463">https://blog.csdn.net/qq_35045096/article/details/91949463</a><br>关于微表面和宏表面的关系以及microfacet distribution function:<br><a href="https://blog.csdn.net/weixin_33856370/article/details/85929381">https://blog.csdn.net/weixin_33856370/article/details/85929381</a><br>菲涅尔准则:<a href="https://blog.csdn.net/xuehuic/article/details/6229532?locationNum=14">https://blog.csdn.net/xuehuic/article/details/6229532?locationNum=14</a><br>(菲涅尔准则是用于衡量反射光的柔和程度的)<br><img src="https://img-blog.csdnimg.cn/822b43356a67486394e009186801b215.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_20,color_FFFFFF,t_70,g_se,x_16" alt="GGX"></p><h5 id="3-3-Lumitexel"><a href="#3-3-Lumitexel" class="headerlink" title="3.3 Lumitexel"></a>3.3 Lumitexel</h5><p>由于上式关于$L(l)$的线性性,从而可以表达为,m即称为Lumitexel<br>$$<br>B(I, \mathbf{p})=\sum_{l} I(l) m(l ; \mathbf{p})$$<br>根据论文:<br>Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter<br>Seidel. 2003. Image-based Reconstruction of Spatial Appearance and Geometric<br>Detail. ACM Trans. Graph. 22, 2 (April 2003), 234–257</p><p>m is a function of the light source j,defined on each point p of the physical sample:<br>$$<br>m(j ; \mathbf{p})=B({I(l=j)=1, I(l \neq j)=0}, \mathbf{p})<br>$$<br>后面那个符号的含义是,其他的灯都关上,打开这个等,并且$I(l)=1$,在本文中 lumitexel会被当成主要的数据结构</p><h5 id="3-4-Problem-Formulation"><a href="#3-4-Problem-Formulation" class="headerlink" title="3.4 Problem Formulation"></a>3.4 Problem Formulation</h5><p><strong>“</strong><br>Problem Formulation. From Eq. 1&amp;2, for a point $\mathrm{p}$ on the physical sample, reflectance acquisition is essentially to solve for the unknown BRDF $f_{r}$ and its local frame, parameterized as ${\rho_{d}, \rho_{s},\alpha_{x}, \alpha_{y}, \mathbf{n}, \mathbf{t}}$, from the photographs ${B(I, \mathbf{p})}_{I}$ captured with predetermined lighting patterns {${I(l)}_l$}. All other variables involved in Eq. 1 can be pre-calibrated.<br><strong>“</strong><br>n是法向量,t是切向量构成了local frame.</p><h3 id="4-FRAMEWORK"><a href="#4-FRAMEWORK" class="headerlink" title="4.FRAMEWORK"></a>4.FRAMEWORK</h3><p>本文提出了一个对于lumitexels的autoencoder(L-DAE)用来为每一个点p上的m进行编码和解码(对于给定数量的lighting pattern).然后fit a 4D BRDF along with the local frame to the lumitexel(<strong>?</strong>).这个过程为样本上的每一个点执行,产生描述6D SVBRDF的texture maps.<br>基本上来看分为两步,第一步是得到编码后的lumitexel然后解码,第二步是一个分离开的BRDF fitting step 用来得到final 4D BRDF.</p><h3 id="5-L-DAE"><a href="#5-L-DAE" class="headerlink" title="5. L-DAE"></a>5. L-DAE</h3><p><strong>“</strong> In acquisition, it is applied to each of the RGB channels to obtain an RGB lumitexel as the result.<strong>“</strong></p><ul><li>DAE由两部分组成: a nonnegative, linear encoder, and a stacked, nonlinear decoder.其中encoder是通过将lighting pattern打到物理样本上,然后测量反射光,它的本质上其实是: <strong>“</strong> performing dot products between the lumitexel and the lighting patterns <strong>“</strong>.而lighting pattern对应的是encoder中的weight.对于decoder来说,它是一个堆叠的,宽度增加的network它的结构是这样的:<br><img src="https://img-blog.csdnimg.cn/15d16615402e40b4984d44c6c43a45c1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbGl6aGlxaV9jcmVhdG9y,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></li><li>encoder实际上是一个convolutional layer with no padding.卷积核是 $c\times1\times$# 的c是lumitexel的维度,#是lighting pattern的数量.</li><li>decoder network接在encoder后面,有11层全连接层,使用全连接层的原因是为了避免作出”lumitexel中不同元素在空间上联系”的假设.每一个全连接层前面都有一个batch normalization层,其后有一个leaky ReLU激活函数(第一个全连接层前没有bn layer)</li><li>这个L-DAE与传统的conventional autoencoder有一些不同.首先,encoder和handware上实现的physical acquisition process是对应的(实际上就是一个物理过程),这些排除了复杂的操作,只剩下lumitexel以及lighting patterns上的非负的乘法和加法.而decoder由于跑在计算机上就没有这个限制,所以使用stacked nonlinear neural network作为decoder,来利用现代深度学习的技术.</li></ul><p><strong>感觉它的流程应该是这样的,首先通过技术手段测量得到lumitexel,这个东西与光线的lighting pattern无关(说起来是pattern实际上就是不同位置光线的强弱),然后就用一些光取照,然后把得到的这B(I,p)放到decoder里面去训练神经网络.需要注意的是lighting pattern是不变的,他们训练的是,对于不同的lumutexel,与lighting pattern计算出来的东西,解码过后能够得到该lumitexel.</strong></p><h5 id="5-1-Loss-Function"><a href="#5-1-Loss-Function" class="headerlink" title="5.1 Loss Function"></a>5.1 Loss Function</h5><p> $$<br>L=L_{\text {auto }}(m)+\lambda \sum_{w \in \text { enc. }} L_{\text {barrier }}(w)<br> $$<br>第一项是重建出的m与ground truth之间的error.<br>$$<br>L_{\text {auto }}(m)=\sum_{j}\left[\log (1+m(j))-\log \left(1+m_{\mathrm{gt}}(j)\right)\right]^{2}<br>$$<br>使用log取解决镜面lobe中的 possible large value,这种做法与下面的论文类似:<br>Jannik Boll Nielsen, Henrik Wann Jensen, and Ravi Ramamoorthi. 2015. On Optimal,<br>Minimal BRDF Sampling for Reflectance Acquisition. ACM Trans. Graph. 34, 6,<br>Article 186 (Oct. 2015), 11 pages.<br>第二项是用来确定计算中的lighting pattern在物理上的可信性.<br><strong>“</strong> It penalizes any weight w in the encoder that is beyond the range of [0, 1], as w corresponds to the ratio of the lighting intensity over its maximum intensity for each source <strong>“</strong><br>很疑惑,这个w对应的是对lighting pattern的惩罚项,那它该怎么计算?训练的时候怎么会训练到它?它不是一个系数项是人为控制的吗?<br>$$<br>L_{\text {barrier }}(w)=\tanh \left(\frac{w-(1-\epsilon)}{\epsilon}\right)+\tanh \left(\frac{-w+\epsilon}{\epsilon}\right)+2<br>$$<br>We find that $\lambda=0.03, \epsilon=0.005$ works well in our experiments.</p><h5 id="5-2-Training-Data"><a href="#5-2-Training-Data" class="headerlink" title="5.2 Training Data"></a>5.2 Training Data</h5><p>训练数据是通过渲染方程模拟出来的.using a large number of randomly generated fr ,the local frame and the location on the physical sample<br>(光源位置不是变量,因为那个仪器那些是固定的,就像一个盒子一样)<br>下面是样本生成的方法:<br><strong>“</strong> Specifically, for the local frame, we randomly sample n in the upper hemisphere of the sample plane, and then t as a random unit vector that is orthogonal to n. Similarly, for the location on the<br>physical sample, we randomly choose a point from the valid region of the sample plane. For the BRDF fr , we use the anisotropic GGX model and randomly sample ρd /ρs uniformly in the range of [0, 1], and αx /αy uniformly on the log scale in the range of [0.006, 0.5]. The calibration data of the acquisition setup (Sec. 6) are used when evaluating Eq. 4 for training lumitexel generation.<strong>“</strong><br><strong>有个问题什么叫sample n in the upper hemisphere of the sample plane?</strong><br>尽管生成数据使用的是GGX model 来生成$f_r$的,但是需要注意的是该模型并不局限在任意的BRDF model上.并且并不要求训练的时候采用的BRDF model和最后结果的是同一个model.<br>(我的理解是因为这个网络实际上就在做类似于解线性方程组的东西,而这个模型中固定的部分只有lighting pattern(即训练的是在某些lighting pattern下解方程的过程)而m是将BRDF作为整体包含在里面的,肯定不会受到,所以肯定不会受到BRDF model的影响)</p><h3 id="6-Acquisition-setup"><a href="#6-Acquisition-setup" class="headerlink" title="6.Acquisition setup"></a>6.Acquisition setup</h3><p>见论文,这篇论文的装置和下一篇论文的装置是一样的.</p><h3 id="7-Implementation-Details"><a href="#7-Implementation-Details" class="headerlink" title="7.Implementation Details"></a>7.Implementation Details</h3><ul><li>训练模型时的back propagation使用的是RMSProp(with mini-batches of 50 and a momentum of 0.9)<br>Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-RMSProp: Divide the Gradient<br>by a Running Average of Its Recent Magnitude. Neural Networks for Machine<br>Learning 4 (2012), 26–31.<br>这个课程观看记录见:</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>3D-reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/01/09/hello-world/"/>
    <url>/2023/01/09/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>or</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new post <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Delete-a-post"><a href="#Delete-a-post" class="headerlink" title="Delete a post"></a>Delete a post</h3><ol><li>first delete the file in _posts</li><li> run:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo clean<br></code></pre></td></tr></table></figure></li></ol><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>or</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo s<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>or</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo g -d<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
